{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCE 633 600 (Machine Learning) Homework 4\n",
    "## Name: Rohan Chaudhury\n",
    "## UIN: 432001358\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Predicting one's hirability based on their job interview\n",
    "\n",
    "Communication skills are essential for successfully entering the workforce. A particularly important communication skill is the ability to connect the job role requirements with one's professional or personal experience, which is commonly asked during job interviews.\n",
    "\n",
    "The goal of this problem is to predict one's job hirability based on their physiological responses\n",
    "and vocal measures during the interview. We have collected data from 13 participants during a\n",
    "job interview with an industry representative, located inside \"Homework 4\" folder on CANVAS\n",
    "(\"data.csv\"). The rows of the file refer to the samples of the dataset, where each interviewee\n",
    "participant corresponds to one sample. The columns denote the participant ID (column 1),\n",
    "participant's physiological and vocal measures (columns 2-9), and the outcome variable (column\n",
    "10), as described below:\n",
    "\n",
    "1. PID: interviewee ID\n",
    "\n",
    "2. SCL: skin conductance level measured as a proxy of the amount of sweat elicited from the body (measure of physiological reactivity)\n",
    "\n",
    "3. SCRAmp: average amplitude of skin conductance responses (measure of physiological reactivity)\n",
    "\n",
    "4. SCRfreq: average number of skin conductance responses (measure of physiological reactivity)\n",
    "\n",
    "5. HRmean: average heart rate (measure of physiological reactivity)\n",
    "\n",
    "6. ACCmean: average wrist acceleration\n",
    "\n",
    "7. Energy: energy of the speech signal (measure of voice loudness)\n",
    "\n",
    "8. ZCR: speech zero-crossing rate (proxy measure of speech rate)\n",
    "\n",
    "9. VoiceProb: voicing probability of speech (measure of speech quality)\n",
    "\n",
    "10. Hirability: hirability score assigned by the interviewer (1-5 scale; 1: low; 5: high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) (2 points) Feature exploration: Compute the Pearson's correlation between each feature and the hirability score (i.e., 8 correlations in total). Which features appear to be the most predictive of the outcome? Please comment on the sign (i.e., positive/negative) of the correlation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import graphviz\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PID       SCL    SCRamp   SCRfreq      HRmean    ACCmean    Energy  \\\n",
      "0  P1  0.827077  0.094424  2.898792   73.276206  66.491987  0.005447   \n",
      "1  P2  5.183890  0.244549  5.219806  102.502828  65.031908  0.005782   \n",
      "2  P3  0.023257       NaN  0.000000   90.603827  64.658708  0.006265   \n",
      "3  P4  0.211715  0.008790  2.299042   80.258785  65.260810  0.006127   \n",
      "4  P5  0.082192  0.011505  1.299459   71.348170  66.607203  0.005081   \n",
      "\n",
      "        ZCR  VoiceProb  Hirability  \n",
      "0  0.120261   0.403078           4  \n",
      "1  0.074726   0.457309           5  \n",
      "2  0.098657   0.427495           5  \n",
      "3  0.091186   0.437544           3  \n",
      "4  0.076909   0.374498           4  \n",
      " \n",
      "Total no. of rows in data.csv: 13 \n",
      "\n",
      " \n",
      "Total no. of colums in data.csv: 10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_train=\"data.csv\"\n",
    "\n",
    "train_rows =pd.read_csv(file_train)\n",
    "\n",
    "print (train_rows.head())\n",
    "print (\" \")\n",
    "print(\"Total no. of rows in data.csv: %d \\n\"%(len(train_rows)))\n",
    "print (\" \")\n",
    "print(\"Total no. of colums in data.csv: %d \\n\"%(train_rows.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's correlation coefficient between Hirability and SCL is : -0.05021625845476406\n",
      "Pearson's correlation coefficient between Hirability and SCRamp is : 0.25910061151032837\n",
      "Pearson's correlation coefficient between Hirability and SCRfreq is : -0.28007354950846525\n",
      "Pearson's correlation coefficient between Hirability and HRmean is : 0.08944574167632677\n",
      "Pearson's correlation coefficient between Hirability and ACCmean is : 0.02948404711128631\n",
      "Pearson's correlation coefficient between Hirability and Energy is : 0.4768001512064216\n",
      "Pearson's correlation coefficient between Hirability and ZCR is : -0.0492520278990791\n",
      "Pearson's correlation coefficient between Hirability and VoiceProb is : 0.2872210071588297\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,train_rows.shape[1]-1):\n",
    "    correlation=train_rows.iloc[:,[i,-1]].corr(method='pearson')\n",
    "    print (\"Pearson's correlation coefficient between {} and {} is : {}\".format(train_rows.columns[-1], train_rows.columns[i], correlation.iloc[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: The Pearson's correlation between each feature and the hirability score is as follows:\n",
    "\n",
    "Pearson's correlation coefficient between Hirability and SCL is : -0.05021625845476406\n",
    "\n",
    "Pearson's correlation coefficient between Hirability and SCRamp is : 0.25910061151032837\n",
    "\n",
    "Pearson's correlation coefficient between Hirability and SCRfreq is : -0.28007354950846525\n",
    "\n",
    "Pearson's correlation coefficient between Hirability and HRmean is : 0.08944574167632677\n",
    "\n",
    "Pearson's correlation coefficient between Hirability and ACCmean is : 0.02948404711128631\n",
    "\n",
    "Pearson's correlation coefficient between Hirability and Energy is : 0.4768001512064216\n",
    "\n",
    "Pearson's correlation coefficient between Hirability and ZCR is : -0.0492520278990791\n",
    "\n",
    "Pearson's correlation coefficient between Hirability and VoiceProb is : 0.2872210071588297\n",
    "\n",
    "\n",
    "The feature **Energy** with Pearson's correlation coefficient value of 0.47 appears to be the most predictive of the outcome value Hirability.\n",
    "\n",
    "\n",
    "Features having positive Pearson's correlation coefficient with Hirability:\n",
    "\n",
    "1. SCRamp\n",
    "\n",
    "2. HRmean\n",
    "\n",
    "3. ACCmean\n",
    "\n",
    "4. Energy\n",
    "\n",
    "5. VoiceProb\n",
    "\n",
    "Features having negative Pearson's correlation coefficient with Hirability:\n",
    "\n",
    "1. SCL\n",
    "\n",
    "2. SCRfreq\n",
    "\n",
    "3. ZCR\n",
    "\n",
    "\n",
    "In statistics, the Pearson correlation coefficient is a measure of linear correlation between two sets of data. It is the ratio between the covariance of two variables and the product of their standard deviations and the result always has a value between âˆ’1 and 1.\n",
    "\n",
    "\n",
    "**A positive Pearson's correlation coefficient between two variables indicates a positive association between them; that is, as the value of one variable increases, so does the value of the other variable.**\n",
    "\n",
    "**A negative Pearson's correlation coefficient between two variables indicates a negative association between them; that is, as the value of one variable increases, the value of the other variable decreases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) (3 points) Decision tree modeling: Use a decision tree to estimate each interviewee's hirability score based on their physiological and vocal measures. Use a leave-one-sample-out cross-validation (i.e., equivalent to leave-one-participant-out in this case), according to which you will have as many folds as the number of samples (i.e., participants). In each fold, you will use one participant as the test and the rest for training the decision tree. Please collect the estimated hirability score on the test sample from each fold. After all folds are done, please report the average absolute error across all participants (i.e., by taking the estimated hirability value of the test participant from each fold). Experiment with various hyper-parameters (e.g., tree depth) and feature combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing hirability scores for 1 hyperparameter and feature combination:\n",
      " \n",
      "Predicted Hirability score using Decision Tree for sample P1 (test data in this fold) with original hirability score of [4] is: [3.]\n",
      "Predicted Hirability score using Decision Tree for sample P2 (test data in this fold) with original hirability score of [5] is: [5.]\n",
      "Predicted Hirability score using Decision Tree for sample P3 (test data in this fold) with original hirability score of [5] is: [3.]\n",
      "Predicted Hirability score using Decision Tree for sample P4 (test data in this fold) with original hirability score of [3] is: [5.]\n",
      "Predicted Hirability score using Decision Tree for sample P5 (test data in this fold) with original hirability score of [4] is: [3.]\n",
      "Predicted Hirability score using Decision Tree for sample P6 (test data in this fold) with original hirability score of [4] is: [4.]\n",
      "Predicted Hirability score using Decision Tree for sample P7 (test data in this fold) with original hirability score of [5] is: [3.]\n",
      "Predicted Hirability score using Decision Tree for sample P8 (test data in this fold) with original hirability score of [4] is: [5.]\n",
      "Predicted Hirability score using Decision Tree for sample P9 (test data in this fold) with original hirability score of [4] is: [5.]\n",
      "Predicted Hirability score using Decision Tree for sample P10 (test data in this fold) with original hirability score of [5] is: [4.]\n",
      "Predicted Hirability score using Decision Tree for sample P11 (test data in this fold) with original hirability score of [5] is: [4.]\n",
      "Predicted Hirability score using Decision Tree for sample P12 (test data in this fold) with original hirability score of [3] is: [4.]\n",
      "Predicted Hirability score using Decision Tree for sample P13 (test data in this fold) with original hirability score of [5] is: [4.]\n",
      " \n",
      "Average absolute error across all participants using Decision Tree with no limit on tree depth, criterion as \"mae\", splitter as \"best\", and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [1.07692308]\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.07692308])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature_set_1=[\"SCL\",\"SCRamp\",\"SCRfreq\",\"HRmean\",\"ACCmean\",\"Energy\",\"ZCR\",\"VoiceProb\"]\n",
    "# X_matrix_1=train_rows.loc[:,feature_set_1]\n",
    "# X_matrix_1=X_matrix_1.to_numpy()\n",
    "# Y_matrix_1=train_rows.loc[:,\"Hirability\"]\n",
    "# Y_matrix_1=Y_matrix_1.to_numpy()\n",
    "\n",
    "\n",
    "def decision_tree(X_train,Y_train, tree_depth=None, plot_tree=False, criterion=\"mae\", splitter=\"best\", feature_set=None):\n",
    "    if criterion==\"squared_error\":\n",
    "        if tree_depth:\n",
    "            clf = tree.DecisionTreeRegressor(max_depth=tree_depth, random_state=2, splitter=splitter)\n",
    "        else:    \n",
    "            clf = tree.DecisionTreeRegressor(random_state=2, splitter=splitter)\n",
    "    else:\n",
    "        if tree_depth:\n",
    "            clf = tree.DecisionTreeRegressor(max_depth=tree_depth, random_state=2, criterion=criterion, splitter=splitter)\n",
    "        else:    \n",
    "            clf = tree.DecisionTreeRegressor(random_state=2, criterion=criterion, splitter=splitter)\n",
    "    clf = clf.fit(X_train,Y_train)\n",
    "    if plot_tree:\n",
    "        tree.plot_tree(clf,feature_names = feature_set, \n",
    "               class_names=['1','2','3','4','5'],\n",
    "               rounded=True, \n",
    "               filled = True)\n",
    "    return clf\n",
    "\n",
    "\n",
    "\n",
    "def get_score_dt(train_rows, feature_set=[\"SCL\",\"SCRamp\",\"SCRfreq\",\"HRmean\",\"ACCmean\",\"Energy\",\"ZCR\",\"VoiceProb\"], \n",
    "                 tree_depth=None, print_hirability=False, print_avg_score=True, criterion=\"mae\", splitter=\"best\"):\n",
    "    X_matrix=train_rows.loc[:,feature_set]\n",
    "    X_matrix=X_matrix.to_numpy()\n",
    "    Y_matrix=train_rows.loc[:,\"Hirability\"]\n",
    "    Y_matrix=Y_matrix.to_numpy()\n",
    "    X_matrix = np.nan_to_num(X_matrix)\n",
    "    scores=[]\n",
    "    abs_err=[]\n",
    "#     print (X_matrix)\n",
    "    for i in range(0, len(Y_matrix)):\n",
    "        if (i!=0):\n",
    "            X_train=np.concatenate((X_matrix[:i],X_matrix[i+1:]))\n",
    "            Y_train=np.concatenate((Y_matrix[:i],Y_matrix[i+1:]))\n",
    "            X_test=X_matrix[i:i+1]\n",
    "            Y_test=Y_matrix[i:i+1]\n",
    "        else:\n",
    "            X_train=X_matrix[1:]\n",
    "            Y_train=Y_matrix[1:]\n",
    "            X_test=X_matrix[:1]\n",
    "            Y_test=Y_matrix[:1]\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print (X_train)\n",
    "\n",
    "        clf=decision_tree(X_train,Y_train,tree_depth=tree_depth, criterion=criterion, splitter=splitter, feature_set=feature_set)\n",
    "#         trees.append(clf)\n",
    "        pred=clf.predict(X_test)\n",
    "        if print_hirability:\n",
    "            print (\"Predicted Hirability score using Decision Tree for sample P{} (test data in this fold) with original hirability score of {} is: {}\".format(i+1, Y_test, pred))\n",
    "        scores.append(pred)\n",
    "        abs_err.append(abs(pred-Y_test))\n",
    "    \n",
    "    avg_abs_err=sum(abs_err)/len(abs_err)\n",
    "    \n",
    "    if print_avg_score:\n",
    "        print (\" \")\n",
    "        if tree_depth:\n",
    "            print (\"Average absolute error across all participants using Decision Tree with maximum tree depth of {}, criterion as \\\"{}\\\", splitter as \\\"{}\\\", and features in the training set as {} is: {}\".format(tree_depth,criterion, splitter,feature_set,avg_abs_err))\n",
    "\n",
    "        else:\n",
    "            print (\"Average absolute error across all participants using Decision Tree with no limit on tree depth, criterion as \\\"{}\\\", splitter as \\\"{}\\\", and features in the training set as {} is: {}\".format(criterion, splitter,feature_set,avg_abs_err))\n",
    "        print (\" \")\n",
    "    return avg_abs_err\n",
    "        \n",
    "# trees=[]\n",
    "print (\"Printing hirability scores for 1 hyperparameter and feature combination:\")\n",
    "print (\" \")\n",
    "get_score_dt(train_rows, print_hirability=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: The estimated hirability score on the test sample from 1 hyperparameter combination is printed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing only Average Absolute error for the rest of the hyperparameter and feature combinations of the Decision Tree:\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Decision Tree with no limit on tree depth, criterion as \"friedman_mse\", splitter as \"best\", and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.84615385]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Decision Tree with no limit on tree depth, criterion as \"friedman_mse\", splitter as \"best\", and features in the training set as ['Energy'] is: [1.]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Decision Tree with maximum tree depth of 2, criterion as \"friedman_mse\", splitter as \"best\", and features in the training set as ['Energy', 'VoiceProb'] is: [0.71611722]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Decision Tree with maximum tree depth of 2, criterion as \"friedman_mse\", splitter as \"random\", and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.74615385]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Decision Tree with maximum tree depth of 3, criterion as \"mse\", splitter as \"random\", and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [0.94059829]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Decision Tree with maximum tree depth of 4, criterion as \"friedman_mse\", splitter as \"best\", and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.88717949]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Decision Tree with maximum tree depth of 5, criterion as \"mse\", splitter as \"best\", and features in the training set as ['Energy'] is: [0.94871795]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Decision Tree with no limit on tree depth, criterion as \"mae\", splitter as \"random\", and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.92307692]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Decision Tree with no limit on tree depth, criterion as \"mae\", splitter as \"random\", and features in the training set as ['Energy'] is: [1.15384615]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Decision Tree with maximum tree depth of 2, criterion as \"mae\", splitter as \"best\", and features in the training set as ['Energy', 'VoiceProb'] is: [0.69230769]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Decision Tree with maximum tree depth of 2, criterion as \"mse\", splitter as \"best\", and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.9787851]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Decision Tree with maximum tree depth of 3, criterion as \"mae\", splitter as \"best\", and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [0.92307692]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Decision Tree with maximum tree depth of 4, criterion as \"mse\", splitter as \"random\", and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.85714286]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Decision Tree with maximum tree depth of 5, criterion as \"mae\", splitter as \"random\", and features in the training set as ['Energy'] is: [1.15384615]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print (\"Printing only Average Absolute error for the rest of the hyperparameter and feature combinations of the Decision Tree:\")\n",
    "print (\" \")\n",
    "\n",
    "hyperparams=[(None,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"], \"friedman_mse\", \"best\"),\n",
    "             (None,[\"Energy\"], \"friedman_mse\", \"best\"),(2,[\"Energy\",\"VoiceProb\"],\"friedman_mse\", \"best\"),\n",
    "             (2,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"],\"friedman_mse\", \"random\"),\n",
    "             (3,[\"SCL\",\"SCRamp\",\"SCRfreq\",\"HRmean\",\"ACCmean\",\"Energy\",\"ZCR\",\"VoiceProb\"],\"mse\", \"random\"),\n",
    "             (4,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"],\"friedman_mse\", \"best\"),(5,[\"Energy\"],\"mse\", \"best\"),\n",
    "            (None,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"], \"mae\", \"random\"),\n",
    "             (None,[\"Energy\"], \"mae\", \"random\"),(2,[\"Energy\",\"VoiceProb\"], \"mae\", \"best\"),\n",
    "             (2,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"], \"mse\", \"best\"),\n",
    "             (3,[\"SCL\",\"SCRamp\",\"SCRfreq\",\"HRmean\",\"ACCmean\",\"Energy\",\"ZCR\",\"VoiceProb\"], \"mae\", \"best\"),\n",
    "             (4,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"],\"mse\", \"random\"),(5,[\"Energy\"], \"mae\", \"random\")]\n",
    "\n",
    "\n",
    "avg_scores=[]\n",
    "# trees=[]\n",
    "for hyp in hyperparams:\n",
    "#     print (hyp[1],hyp[0])\n",
    "    avg_score=get_score_dt(train_rows, feature_set=hyp[1], tree_depth=hyp[0], criterion=hyp[2], splitter=hyp[3])\n",
    "    avg_scores.append(avg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: After all folds are done, the average absolute error across all participants is calculated for various hyper-parameters and feature combinations of the Decision Trees and shown below:\n",
    "\n",
    "1. Average absolute error across all participants using Decision Tree with no limit on tree depth, criterion as \"friedman_mse\", splitter as \"best\", and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.84615385]\n",
    " \n",
    " \n",
    "2. Average absolute error across all participants using Decision Tree with no limit on tree depth, criterion as \"friedman_mse\", splitter as \"best\", and features in the training set as ['Energy'] is: [1.]\n",
    " \n",
    " \n",
    "3. Average absolute error across all participants using Decision Tree with maximum tree depth of 2, criterion as \"friedman_mse\", splitter as \"best\", and features in the training set as ['Energy', 'VoiceProb'] is: [0.71611722]\n",
    " \n",
    " \n",
    "4. Average absolute error across all participants using Decision Tree with maximum tree depth of 2, criterion as \"friedman_mse\", splitter as \"random\", and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.74615385]\n",
    " \n",
    " \n",
    "5. Average absolute error across all participants using Decision Tree with maximum tree depth of 3, criterion as \"mse\", splitter as \"random\", and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [0.94059829]\n",
    " \n",
    " \n",
    "6. Average absolute error across all participants using Decision Tree with maximum tree depth of 4, criterion as \"friedman_mse\", splitter as \"best\", and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.88717949]\n",
    " \n",
    " \n",
    "7. Average absolute error across all participants using Decision Tree with maximum tree depth of 5, criterion as \"mse\", splitter as \"best\", and features in the training set as ['Energy'] is: [0.94871795]\n",
    " \n",
    " \n",
    "8. Average absolute error across all participants using Decision Tree with no limit on tree depth, criterion as \"mae\", splitter as \"random\", and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.92307692]\n",
    " \n",
    " \n",
    "9. Average absolute error across all participants using Decision Tree with no limit on tree depth, criterion as \"mae\", splitter as \"random\", and features in the training set as ['Energy'] is: [1.15384615]\n",
    " \n",
    " \n",
    "10. Average absolute error across all participants using Decision Tree with maximum tree depth of 2, criterion as \"mae\", splitter as \"best\", and features in the training set as ['Energy', 'VoiceProb'] is: [0.69230769]\n",
    " \n",
    " \n",
    "11. Average absolute error across all participants using Decision Tree with maximum tree depth of 2, criterion as \"mse\", splitter as \"best\", and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.9787851]\n",
    " \n",
    " \n",
    "12. Average absolute error across all participants using Decision Tree with maximum tree depth of 3, criterion as \"mae\", splitter as \"best\", and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [0.92307692]\n",
    " \n",
    " \n",
    "13. Average absolute error across all participants using Decision Tree with maximum tree depth of 4, criterion as \"mse\", splitter as \"random\", and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.85714286]\n",
    " \n",
    " \n",
    "14. Average absolute error across all participants using Decision Tree with maximum tree depth of 5, criterion as \"mae\", splitter as \"random\", and features in the training set as ['Energy'] is: [1.15384615]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) (2 points) Decision tree visualization: Provide a plot of the root, nodes, and decisionboundaries of the best decision tree. Provide your intuition regarding the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Decision tree with minimum average absolute error of [0.69230769] has maximum tree depth of 2, criterion as \"mae\", splitter as \"best\", and following features in the training set: ['Energy', 'VoiceProb']\n",
      " \n",
      "Decision tree visualization for the best decision tree:\n",
      " \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3yT1f7A8c9JOtNCC7RM4QJKGaWWcZUVoBQQijLKEJEChYJeBz/l4kVQLwKCBa8DKKBXWQoXwcv2IirDUpAhoMyy9yqbrnSkyfn9kTY0tNCCTdJx3q9XXtInT8/zzTH95uQ8ZwgpJYqiKIpjaJwdgKIoSlmikq6iKIoDqaSrKIriQCrpKoqiOJBKuoqiKA6kkq6iKIoDqaSrKIriQCrpKoqiOJBKuoqiKA6kkq6iKIoDqaSrKIriQCrpKoqiOJBKuoqiKA6kkq6iKIoDqaSrKIriQCrpKoqiOJBKuoqiKA6kkq6iKIoDuTg7AKXkEEI0ARqi3je5SeA8sE1KaXZ2MErxp/54lEJxd3d/w9fX90O9Xm/y8PAQzo6nuMjKypJ79+7V3L59e40QYpBKvEpBhNqYUimIEKJR+fLl9+zfv9+zdu3azg6n2ElJSaFly5aphw8f/puUcrGz41GKN9WnqxRGQPPmzTNVws2ft7c3ffv21Wm12gbOjkUp/lTSVQpD6+7ubu1SiI2NpXbt2oSEhBASEsLmzZudGVuhXbx4kdDQUFq3bs2mTZvyPD9t2jT0ej0REREYjcb7HtuwYQOdOnWiXbt27N27FwB3d3eh1WrdHPdqlJJKJV3lkURGRhIbG0tsbCyhoaEP/ftmc9F0fRoMhkKfO23aNKKjo/n555/58MMPbZ67du0a27ZtY9u2bQQFBfH999/neywtLY0FCxbw008/ERcXR/PmzYvkdShlh0q6SpFYuHAhERERdOvWjbCwMMxmMwkJCTz77LO0b9+eDz74ALAk69dff53u3buza9cumjdvzosvvkizZs0A6NSpEzn3GZ599tl8k6qUkvXr19O7d28++uijQscYHx9PixYt8Pb2RqfT2ZS9e/duOnToYI1h586d+R7bvn07UkrCwsKIjIwkLS3t0SpMKbNU0lUeycKFC63dCwcPHgSgSpUq/PDDDzz22GMcPHiQqVOnMmnSJLZs2cLRo0e5cuUKAB06dGDdunVMnjyZ77//nrlz53LhwgUA2rVrx7Zt20hISMDX1xedTme9ZnJyMh999BGhoaHs2bOHWbNmMWHCBADef/99azw5j5yv/jlyt659fHy4c+eO9ec7d+5Qvnx5m+fyO3bt2jUSEhJYv349LVq0YO7cuUVcs0ppp4aMKY8kMjLSmvAA9u7dS+PGjQGoUaMGd+7c4dixY4wePRqwJLVLly4B0LRpU8By17969eoAPPHEEwAMGDCAmTNnUr9+ffr27WtzzcuXL/P111/Tv39/hg8fTrVq1azPTZw4scCYNZq7bYykpCR8fX2tP/v6+nL16lWb5/I75uPjg16vR6vVEhoayvTp0wtRW4pyl2rpKkVGiLvDd6WUBAQEEBMTQ2xsLHv37rV2IeQkP29vby5fvozBYODkyZMA1KtXj7Nnz7Ju3TrCwsJsyq9fvz6HDh3i6aef5vXXX+f555/n119/BQrX0g0MDGT37t2kpqZiMBhsWtFPPfUUW7ZsAWDjxo20bNnyvseOHDkCwL59+6hbt26R1Z9SNqiWrvJIFi5cSGxsLABvvPFGvueMGzeOESNGkJqaiqurKytWrLB5/r333qN79+488cQT1KxZ03q8Q4cO7Nu3Dw8PjzxlCiHo2rUrXbt25fz58/zxxx9A4Vq6Y8aMYfDgwaSnpzNp0iTr62jSpAlNmjShdevW6PV6ateuzahRo3B1db3vsfbt2+Pt7c2SJUsKVV+KYiWlVA/1eOAD6NO1a9dEWcQyMzOllFImJyfLVq1aWY9/8sknct26dUV9Obv68MMPpZub20eyGPz/Uo/i/VDdC4rTxMXFodfradeuHWPGjAEgJiaGn376iS5dujg5OkWxD9W9oDhNx44d6dixo82xkSNHMnLkSCdFpCj2p1q6iqIoDqSSrlIqGI1GIiIi0Ov1TJs2Lc/z586do3v37oSEhFjH1o4aNQp/f38WLlxoPS/36IeSMr1ZKVlU94JSKqxdu5agoCAWL15M9+7dGTp0KJUrV7Y+P378eL7++msqVqxoPfb2228THBycp6ycURmKYg+qpasUudjYWLp27UpYWBhdunQhJiaGVq1a8e677wIwb948OnXqxFNPPcWePXsAWLlyJXq9Hr1ez++///7Q19y5cyedO3cGLK3VnHLB0go+d+4cw4cPJywsjFOnTgFQtWrVPOVoNBo6dOjAwIEDSUxMfOg4FKUgKukqduHt7c369evJWQ5yx44d1okGAwYMYOPGjSxbtoxPP/0Us9nM7Nmz2bJlC99//z1TpkyxKWvVqlV5Jj7MmzfP5pz8puzmuHHjBgcOHODLL7/k448/ZuzYsfeNe/ny5fzyyy88++yzREdHF0VVKIoN1b2g2EVgYCAA1apVs04PzpnssG7dOmJiYtBoNAghuH79OocPH7aOZHBxsX1bhoeHEx4e/sDr+fr6kpSUBFim7OZML855LjAwED8/P/z8/Lhx48Z9y8npfggPD2fxYrUeuVL0VNJV7CL3lODc/wb45JNP2LJlC+fPn2fEiBH4+fkRHBzM+vXr0Wg01nVrc6xatYoZM2bYHBs0aBBRUVHWn1u1asWmTZto1qwZsbGxREREWJ/z9PTEy8uLtLQ0bt26ZW0R5yc5OZly5cqxY8cONcVXsQuVdBWH69KlC+3atbMum6jVannllVcICQlBq9XSoUMHxo8fbz2/MC3dnJtner2e7t27U7lyZfbt28e+ffuIjIxk3LhxPPPMM9auDICpU6eyaNEiABISEhg7diydOnXCzc0NT09PFixYYKcaUMoytUeaUiAhRJ+uXbvOX79+/f2biGVcdHQ0EyZM+FdGRsYYZ8eiFG/qRpqiKIoDqaSrKIriQCrpKsVeZGQkZ8+etUvZ+c1KGzhwIO3ataNNmzYcPXrULtdVyi51I00p0/KblbZw4UJcXV3ZunUrc+bMYebMmU6KTimNVEtXKTLbt2+nRYsW1skLGRkZdOnShbZt2zJ8+HCg4NlqkZGRvPbaa7Rt29ZmOyAg340u33nnHevykJcvX37omPOblebq6gpYho/lbC2kKEVFtXSVIrN+/XrrbhA5m0CuXbsWd3d3Bg8ezPHjxwHLbLXly5fz8ssvA5bZanq93lpOu3btmD17Nt26dSMhIcF6PGejy+bNmzNw4ECuXLnCTz/9xK5du3BxceHekTg9e/bMM5V306ZNaLXaAl9L27ZtuXTpEmvWrHm0ylCU+1BJVykyr776KpMmTeK7775j5MiRNGzYkKioKK5evcq5c+esuwE/aLYa3N24Mjg42KYvN7+NLidPnkxkZCSVKlXiww8/xMvLy3r+n0mYW7duZc+ePUyYMCHPNkOK8meopKsUmQoVKvD5559z5coVXnrpJYYMGUJwcDDvvvsuAwcOtLZEHzRbDWD//v0EBARw4MABm/3XAgICGD58OEFBQZhMJoQQZGZmEhYWRnR0ND/++CN9+vSxnv+oLd3MzEzc3Nzw8fHJd582RfkzVNJViswXX3zB6tWrSU5OZty4cbRo0YLJkyeza9euhypn8+bNzJw5k9DQUJs+1/w2uuzbty8GgwEhhM20YChcSze/WWldunRBSokQgjlz5jxU7IpSIGdv0qYexf+BnTamzM+QIUPkmTNnHHGpIqU2plSPwj7U6AVFURQHUt0LSrGSe5KCopRGqqWrKIriQCrpKsVCSEiI3a+xatUq6tSpk+f4xYsXCQ0NpXXr1mzatMnucShlm+peUMqMlStXUrNmzTzHp02bRnR0NIGBgfTs2dO6g4Wi2INq6Sp2NWLECE6ePAnABx98wObNm/PdmDJH7sVtclq/R44coWPHjuj1eubPn/9IcWzevBm9Xo9Gk/ctHx8fT4sWLfD29kan02EwGB7pGopSGCrpKnbVu3dvVq5cCUBcXBzt27fPszFlQd5//30WL17M1q1bWbZsGVlZWdbnzpw5k2fTypEjR+YpY/78+QwZMiTf8nOmLEPeTS0Vpaip7gXFrjp27MiMGTPo168ftWrVQqvV5tmYMrfcP0tpmcF24sQJBgwYAFh29r1x44Z10kSdOnWIjY19YAx79+6lUaNG951dlrv1m5SUhK+v70O/TkUpLJV0Fbtyc3OjcuXKxMTE0Lt3byDvxpS5+fj4kJCQQNWqVa0L5AQEBDBr1iz8/f0xGo3WVcDA0tIdOnSoTRlBQUHExMRYfz548CAbNmwgLi6OAwcOMGnSJJs92AIDA9m9ezeNGjXCYDCg0+mKvB4UxcrZszPUo/g/+JMz0lauXCkrVKgg09PTpZRSjh8/Xj799NPy7bfflu3bt5dSSut/9+7dK4ODg2VERIRs2rSplFLKI0eOyM6dO8uQkBDZu3fvRw3D5jpSSvnGG29IKaW8cOGC7NChg2zVqpXcsGHDI5WrZqSpR2EfamNKpUBCiN6dO3de8PPPP6uNKe9j8uTJTJw4cZrRaBzr7FiU4k3dSFMK4+KhQ4e0qampzo6jWDKbzcTFxaVlZWWdc3YsSvGnWrpKgYQQGm9v70V169bt+eKLL3q5u7s7O6Riw2QysXnz5vStW7ceTU5ObielTHZ2TErxppKuUihCCA0Q6e7u/qSLi0t+WVeYTKa2UsoqWq32ByFEaWgWa7OysjoB7lqt9kchROa9J5jNZlNaWtopYK5KuEphqKSr/GlCiPLAd4AEni9NyUcIoQU+BroC3aSUZ5wcklLCqT5d5U8RQjwGbAXOAt1LU8IFkFKapJSjgDnAr0KIp50dk1KyqaSrPDIhRBNgB/Af4BUpZVYBv1JiSSljgL8B64QQ4c6ORym5VPeC8kiEEGHAN8BrUsrvnB2PowghmgNrsXQ5TJfqD0h5SCrpKg9NCPEyMBHoLaXc7ux4HE0I8RdgHfAL8KaU0uTkkJQSRCVdpdCyRzBEA+FYbiqddHJITiOE8AWWA2nAACllipNDUkoI1aerFIoQwhNYCrQGWpXlhAsgpbwDdANuAFuEENWcHJJSQqikqxRICOEPbAJMQGcp5U0nh1QsSCkzgWHAKmCHEKKxk0NSSgCVdJUHEkIEYBmh8AswUEqZ7uSQihVpMRl4F9gshOjk7JiU4k0lXeW+hBBtsYzBnSqlfFdKaS7od8oqKeV/gH7Af4QQQws6Xym71I00JV9CiAHADCBCSvmzs+MpKYQQDbCMbFgCjFdDypR7qaSr2BCWrRvGYZkI8JyU8oCTQypxhBCVsYzlPQUMk1JmODkkpRhR3QuKlRDCFfgK6Au0VAn30UgprwEdAA/gZyFERSeHpBQjKukqAAghfLB8La4GtJNSXnZySCWalDINSx/vbmC7EKKuk0NSigmVdBWEELWAbcBJoKca6F80pJRmKeVbwEwsi+W0dHZMivOppFvGCSGaAduBhVjWUSi1i9Y4i5RyDjAc+F4I0cfZ8SjOpW6klWFCiGexJNtXpJTLnRxOqZf9AbcW+Az4VI1sKJtU0i2jhBCvAv8EwqWUO50dT1khhKiJpe98G/B/6ptF2aOSbhmTvWjNR0B3LIvWnHJySGVO9k4bywEj0F/1oZctqk+3DBFC6ID/Ak9jWbRGJVwnkFImAc8Cl4E4IUR1J4ekOJBKuqWcEKK5EKJ39oD9zUA6lkVrbjk5tDJNSmkEXsLyIbhDCBEkhGgmhOjr5NAUO1NJt/SbADTEsmjNBizTetUMqWIge7GcaGAsllXcngY+E0K4ODcyxZ5Un24pJoSoDewDMoBPgaVSynPOjEnJK3vGWg9gKpACvCWlXO3cqBR7US3d0u0ToHz2v98AujgxFuX+nsSy3oU78Bcs+68ppZRq6ZZiQoipWFq5S4Djalxo8SaEqIFlVEk7KeWLzo5HsQ+VdBVFURyoVHTYCyEe9/X1XWwymZ6QUqouk0cghDBqNJpdiYmJEVLKZGfHU9oJITr7eGhnGM2yinB2MMWIEMKogb1JGaYXpZSJzo7HHkp8S1cIUUWn0x2aOHFixfDwcI2rq6uzQyqR0tLSiI6Ozli5cmV8cnLyU2pbcfsRQrTRuWl+ntWnnq5RFR0aodJujrQsM//efjljzcEbx1Mzzc2zh9aVKqUh6fbv3LnzVz///HM5Z8dS0pnNZvz9/Q23bt16Uk2csB+dm/bzv4c89rdX9TWcHUqxJKWk6cd7U66nGFtIKeOdHU9RKw1fxX1r1KihdXYQpYFGo6FixYpGwNfZsZRmbi7Cv5KX+kZ2P0IIKupcTJTS92FpSLpW7dq1Izn5bndkREQEhw8fznPem2++WegyJ0yYQJMmTWjVqhUzZ84s8PyzZ88SGRlZ6PIf1qZNm2jVqhWhoaFcunTJ5rktW7bQqlUrWrVqxbhx4wC4fPky7dq1o02bNtZj3333HW3atKFFixYsXrzYpgyhvuo61PYzibT47Hf6LjhM3wWH2Xa6ZHRjXk7MoN/Cw/SYe5Ct+cQ8e+sles07xMgVJzCazPc9FnfqDv2/jqf3/EMcuHx3CQpB6X0flqqkGxYWxo8//ghAVlYWx48fJzAwMM9506dPf6hyp0+fzvbt21m6dClG490uJrO5aDbHNRgMhT53ypQpbNiwgejoaKZNm2bzXOvWrdmxYwc7duzgt99+49atWyxdupSXX36ZX3/9lT179nDnzh1atmzJtm3b2LZtW6E+SBT7er6JP8uHBrJ8aCD6uj4P/ftmc9F0EaZlFr4bf862y4zrVItvBzUiJu6izXM3Uoz8dj6Z1VGNaVBZx4Zjt/M9lmY0seyPaywZ1JCVwxrzZHXvInkdxV2pSrrh4eGsWbMGsLT62rdvz/79+9Hr9bRs2dLaqgsJCQHg6NGjhISE0L59e+bOnUtqair9+vUjNDSUV155xaZsIQS1a9fm5s2bNG3alAEDBvDZZ58xbdo02rRpQ2hoKOfPnwfg/Pnz9OjRg5YtW3LmzJl8YzUYDMybN4/Q0FA2btxYqNdnMBjw9vbG29ubFi1aEB9v292VcxPRZDJRo0YNypUrR4MGDUhOTsZsNiOEwN3dnVq1aiGEwMXFBXXjsfhZ9sc1Rq44waDFR4hYdASzWXItOZNBi4/QZ/4hPou1JLk3V53k3XWnGbLkKL9fTKbrFwd4bflxunxh2dqu/9fx5NyzGbT4SL5JVUrJ5hO3Gb70GHN+LfwOTcevG2j2WDm83LV4umptyt53OYXWdSxzcto+7sPvF1PyPbbnQgpSQsTiI7y56iRpxrJx77ZUDBnL0aBBA06dOkVWVhZr1qyhf//+jB8/niVLllCtWjX0ej0DBgywnv/OO+/w1VdfUa9ePcxmMzNnzuSFF16gT58+jBs3jt9++816bnp6OidPnsTPz49Lly6xfft2EhMTGTZsGL/++ivbtm3jo48+4q233uL69ets2LCB3377jY8//pjZs2dbyzlz5gwzZswgPj6e/v378/333+Pl5QVAz549SUy0/aq2adMmtFpLl/Xt27cpX7689TmTKe+bdOHChUyePJmuXbvi6upK8+bNGTNmDJ988gl9+/bF09PTeu5XX33Fc8899ydrXfmzvtt3nR1nkwD4oFttAPy8XInpU49/rDnFkWsGlv1xjX+E1uTJ6t68vvwEV5MzAWhdx4cpz1ZiyH+OsvDFBvh4aHn6s98BaPmXcvx2Ppk6FT3w8XDB0+3urY+UDBPf7E5g84k7tKnjw+Rudaha3g2AjzdfYOe5JJsYx3f5i01LNHfjuryHlsR0k7X8pPQsyrlb/l3OXUtSela+x26mGrmWYmTZkEYs3nOVb/deY1jLakVVrcVWqUq6YOnXjYuLY+fOnUyfPp2kpCRq1aoFQJ06dbhx44b13Nu3b1OvXj3AchPp2LFjLF26lJiYGFJSUmjRogVg6QP28fFh9OjRuLi40KBBAzw9Pdm/fz/BwcEA/PWvf2XKlCkABAUFodVqadasGRMnTrSJb8+ePWzevJlRo0bRv39/dDqd9bmcVvr9VKhQgaSku38MOck4t8jISAYPHkx4eDgHDx5k0aJFTJkyhR49etCnTx/Onj1L7dq12bNnD//73/9YtWpVoetWsY/nm/gzukNN688HLqfSoIrlfVG1vBtJ6VmcvpHOpJ8sy2YkpmeRkGRJuo2rWj6wUzNN1qRZu6IHAD2D/Ji/M4HH/Tx4NtB2Q+KE5Ez+u+86PRpX4sXmlalSzs363FuhNSmIJleXa3KGCR+Pu+/F8h4uXE9JAyzJvbyHS77HyrlrebpWObQaQZu6PszdcaUQtVXylbqk26tXL8aOHUtwcDAajYZy5cpx/vx5qlWrxpkzZ/Dz87Oe6+vry8mTJ3niiScwm80EBATQrVs3unfvDlj6hfft28f06dOtXRJgSdAAtWvXZv/+/YAlmdata9nw9dChQ5jNZv744w8ef/xxm/j69evHs88+y7fffkuPHj1o2LAho0ePpnbt2gW2dHU6HQaDgdTUVA4fPkyjRo1szs3MzMTNzQ2NRoO3tzceHh5IKalYsSJCCHx8fEhKSuL69euMGjWK1atX55u4leJFSqhbyYMBzSvTsIoXJrO03mbKSX5ebloSkjLx8dBy9lY6AHUreXLhTgZnb6Uz94X6NmU+4efJ5teCiT15h/fWnUGjEQxvWZWnapUvVEu3fmUd+y6lEODvSZrRbNOKblLdm//sucrLrauz9XQizR7zzv9YDW+W/nENgMMJqfylgnsR11zxVOqSbsuWLTlx4gRjxowBYOLEibzwwguYTCZee+01myQzefJkhg0bhkajYdCgQbz00ktERUXx2WefodFo+Oqrrx54rapVq9K2bVtat26Nm5sbX3/9NVJK/Pz86NGjBzdu3ODbb7/N83s6nY6oqCiioqLYvXs3CQkJ1K5du8CWLsDYsWPp3LkzHh4efPPNN4ClJT59+nSWLl3K/PnzycrKIiQkhHr16vHKK68QGRmJlJKAgACefPJJ3n77bS5fvkyfPpY9En/++Wfc3NwedFnFjnJ3L0S1rJrvOa+3rcE/1p7GYDThqhF81d82ib7RvgaRS45Su6IH1X3uJq/WdcpzOCEVD9e8t2+EEHSoV4EO9Spw6U4GhxJSgcK1dF9pU503Vp0kI8vMW9mt9GV/XCOwqheNq3nx15rl6DXvEDV93RnRqhquWs19j/WZfwidm5bZfesVrsJKuNIwOeLlyMjITxcsWKAr+GylIAEBAYknTpzoKKXc6+xYSitfncvy97vU7tO/aeUiK9NoMuOq1ZCaYWLAonjWDg8C4N/bL/OEnycdAyoU2bUcoePs/YlHrxm6SSm3OzuWolbqWrqKUhbtPJfMJ79cIM1oZlT7xwCYv/MKW07eYXgZuDlVkqikqyilQNu6PrS9Z4zvsJbVysRogJKmVI3TLUmMRiMRERHo9fo8kxwABg4caJ1JdvTo0Yc6pih/ltFkZuSKE/Sad4jZWy/le8615Ewe/2AnF26nYzZLBi6yzCyLWHSE5HTLzvL/2XOVHnMPErnkqPVYWaeSrpOsXbuWoKAg68ywa9eu2Ty/cOFC4uLimDp1KnPmzHmoY4ryZ/187DYNKutYHdWY384ncyMl72Jf83cl0KSGZUSDEPBR98dZOawxYQ0rsmL/DYwmM//df53VwxrzQtPKLPn9Wp4yyiKVdIHY2Fi6du1KWFgYXbp0ISYmhlatWvHuu+8CMG/ePDp16sRTTz3Fnj17AFi5ciV6vR69Xs/vv//+0NfcuXMnnTt3Biwz5HLKzZEzUyw5OZmmTZs+1DGldNl+JpGBi+KJWHSEF7+JZ/7OK3T/6iBTN1pmQH679yr9v46n278PsP+SZf2CH+Jv0mveIXrNO8TBXGsaFNbvF5Jp97ilu6JV7fLsv6eMxLQsbqdlUbOCZUywEIIavpZREy5agatWcNuQRQ0fNzQaQYMqOn6/+PBxlEaqTzebt7c3y5cv5+WXXwZgx44d6PV6AAYMGEBUVBSnT5/mvffeY/HixcyePZstW7aQlJTE8OHDWbFihbWsVatWMWPGDJvyBw0aRFRUlPXnO3fuWGeX+fj4cOfOnTwxtW3blkuXLtkMJSvsMaV00blp+ap/fcastay4+f2IIHrNOwRAryA/BjSvwrlb6Xy0+Twxveux8LcEVgwNJDnDxD/WnOKrXON01x+5ybydCTbl93nSUkaOpHQT3tkzyMp7WGaQ5bbwtwQG/bUKc3faTmgwZJpYsvcaiyIa4OWm5dztDNKNZnaeTVLdC9lU0s2WszBOtWrVaNy4MQAeHpZP8XXr1hETE4NGo0EIwfXr1zl8+DAdO3YEwMXFthrDw8MJDw9/4PV8fX2ts8uSkpKoXr16nnO2bt3Knj17mDBhgjWpF/aYUrrU97eMiKxSzo362bPV3F0sX1Q3nrjDgl1XrIuh3zQYOX49jf5fW9bm0GpsV+wKa1iJsIaVHni98h5aUjIs08yTM0w2M9bSjCZOXE/jjexRErm9teYU/witSXkPy9/Eq22qM3BRPE9W90YtZ2mhkm623Esa3ru84SeffMKWLVs4f/48I0aMwM/Pj+DgYNavX49Go7FZeQwK19Jt1aoVmzZtolmzZsTGxhIREWFzfs7sMh8fH2vyL+wxpfTJ/Za8d9HDf2+/zIqhgVxKzGDM2tNU1LnSqIqOxREN0WiEdRnFHIVp6TavWY5tpxMJqu7NjrNJ9HnS3/rcxTsZnL2dzsBF8Ry9auBmqpFFEQ2ZtfUST1b3tlkprVujSnRrVInVB2+gWKikWwhdunShXbt2dOjQAbCsefDKK68QEhKCVqulQ4cOjB8/3np+YVq63bt3Z+jQoej1erp3707lypXZt28f+/btIzIyki5duiClRAhhvUFW2GNK2RLyuC995h+2ruKl1QgGP1WVvgsPoxGCNnXKMyrk7iyzwrR0O9evwN9Xn6LXvEN0DqiAn7crh66kcjghlf5NK/O/EZbJF2+uOrS9PJkAACAASURBVMnokMdIzTDxyS8XaF6zHBuP36ZXkB8Rf63Ce+vOcOy6gfr+OiaG1bZbHZQkakaaYkPNSLM/e8xIK21K84w0NXpBURTFgVTSVRRFcSCVdBVFURxIJV07i4yM5OzZs3Yp+9y5c3Tv3p2QkBDmzp1rl2sopdubq05y4Xa6Xcoe/J8j9F1wmC5fHGDYt2qKeg41eqEEGz9+PF9//TUVK1Ys+GRFcbBvBjYELKud5YwpVlTStdq+fTujRo3C09OTQYMGERERQY8ePTAYDNSvX5+5c+cSGxvL1KlTEUJgNpt57rnnWLJkCaGhoUyZMoXIyEi8vLw4cOAAHTt2ZMKECdbyExISiIqKIiUlhU6dOvHPf/6Td955h7i4ODQaDUuXLs13gsT9GI1Gzp07x/Dhw0lLS2PWrFl5dqlQSo/d55OZ+ONZPFw19HnSj97B/gxdcpQ0o5nH/Tz5uOfjbD+TyOxtlxAIzFLSKaACqw7eoE0dH8Z2qsWbq06ic9Vw5KoBfV0fmy2CriVnMnrNKQyZJvR1fRkV8hjRG8+z61wSGgFz+gZYtwN6WBuO32Zm7yeKqipKPJV0s61fv5733nuP7t27W7dWX7t2Le7u7gwePJjjx48DD54uDJY92mbPnk23bt1ISLg7AH3q1KlMmjSJ5s2bM3DgQK5cucJPP/3Erl27cHFx4d6hewVt3XPjxg0OHDjA8ePHuXr1KmPHjuW///1v0VeMUiz8cuI2/9e+Bs/Ur2jdcn3Biw1wd9HwfytPcOqGZf+xB00XBmhRuzwfPleXQYuPcC17c0uAWdsu5dn4csvJO/xvRBAuWpHn/Tl0yVGSM2w3Rl02pFGe2W930rIwmiT+3mpnkhwq6WZ79dVXmTRpEt999x0jR46kYcOGREVFcfXqVc6dO8eVK5Y55g+aLgxYF50JDg626cs9duwYo0ePBizrLly6dInJkycTGRlJpUqV+PDDD627AkPBm1T6+voSGBiIn58ffn5+NhtuKqXPkKer8lnsRb4/dJNhLapSz1/H6DWnuJGSycXETK6lWBLog6YLw92NLBtV0XHhTob1eH4bX47pWJNRq09SwdOFsZ1qocu1D9qCFxsUKu6Nx2/TqYTtWmFvKulmq1ChAp9//jlXrlzhpZdeYsiQIQQHB/Puu+8ycOBA6yf9g6YLA+zfv5+AgAAOHDjAG2+8YT0eEBDA8OHDCQoKwmQyIYQgMzOTsLAwoqOj+fHHH617lkHBLV1PT0+8vLxIS0vj1q1bNluzK6WPj4cLU7vX5WpyJmPWnqZfE38aVdHxxvMBvL78BDkN0QdNFwaIv2rgcT9P4q8aiMq1wHl+G19mmiSh9SoQE3eJX07e4dlGd2exFbal+9PRW7zTqdafffmlikq62b744gtWr15NcnIy48aNo0WLFkyePJldu3Y9VDmbN29m5syZhIaGUrXq3U0Gx40bx4gRI0hNTcXV1ZUVK1bQt29fDAYDQgibdRmg4JZuTpnPPPMMZrOZ2bNnP1ScSsmyaM9Vfjp6i5QME6+3rUHTx7yZEXeRPy493HKJv55OZP7OK7Sp40PlXIvY5Lfx5UvfHSPNaEYAA5rZboRZmJZuutHMlaRM6lTyfKgYSzs1DbgIRUZGMmHCBGrXru3sUB6ZmgZsf86aBpyzTkLOGrjFmZoGrCiKohQJ1b1QhBYuXOjsEBTlvqaHq2FbxYFq6T6ikJAQu19j1apV1KlTJ8/xixcvEhoaSuvWrdm0aZPd41BKlr4LDtu1/IbRv9F3wWH6LjjMbYPtWtKXEzPot/AwPeYeZOvpxPuUULaplm4xtnLlSmrWrJnn+LRp04iOjiYwMJCePXtad7BQFEcIrOrF8qGB+T43Z9tlxnWqRX1/HUO/PZpnW3hFtXTzGDFiBCdPngTggw8+YPPmzfluTJkj99oKOa3fI0eO0LFjR/R6PfPnz3+kODZv3oxer0ejyfu/KD4+nhYtWuDt7Y1Op8NgMDzSNZSS5R9rTnHmpmUSxGexF9l2OjHfTSlz5F5XIaf1e+K6gecXHqbXvEMsfcTdeY9eMxA+7xAfbTqf57nj1w00e6wcXu5aPF21pGWa8imhbFNJ9x69e/dm5cqVAMTFxdG+fXsGDBjAxo0bWbZsGZ9++mmBZbz//vssXryYrVu3smzZMrKy7m7Id+bMGUJCQmweI0eOzFPG/PnzGTJkSL7l58yYg/tvaqmUPmGNKrL+yC0Adp1LolXt8vQK8mPZkEZ83i+AL3dcLrCMj3+5SEyfeqwaFsjaQzfIMt0dvXT+drq12yDn8d66M3nK2DqyCSuHBXIj1cgvJ27bPGfONRiqvIeWxHSVdO+luhfu0bFjR2bMmEG/fv2oVasWWq02z8aUueX+OWf43YkTJxgwYABgma5748YN65jdOnXqEBsb+8AY9u7dS6NGje6751nu1m9SUhK+vr4P/TqVkkdfx4d5O6/wXGAlavi4odWIPJtS5pb7SM5788zNNF5bfgKAWwYjtwxG63jdWhU87tttkFsFnWWDya4NK3LkqoEO9e7OOMs9NyI5w4SPh/beXy/zVNK9h5ubG5UrVyYmJobevXsDeTemzM3Hx4eEhASqVq1qXZ8hICCAWbNm4e/vj9FoxNX17i6oZ86cYejQoTZlBAUFERMTY/354MGDbNiwgbi4OA4cOMCkSZNs9mALDAxk9+7dNGrUCIPBgE7n9CHKigO4uWio5OXK/F0JhGXPDrt3U8rcynlouZZixN/bjdM3Ld0MdSt5MuXZOlTycsVoMuOqvfsBfv52On9ffcqmjAaVdUx+9u7N3LRME24uGrQawd4LyTSs4mVzfv3KOvZdSiHA35M0oxlPN5V076WSbj7Cw8OJiooiOjoayLsxZW6DBw9m2LBhBAUFUa2aZVrlxIkTGThwIEajkYoVK9psi16Ylm5kZCSRkZGApZ84J+G++eabTJ8+nTFjxjB48GDS09OZNGlSEbxipaQIa1iRt9acYlz21Np7N6XMrV+wP39fc4oGlXXW1uxbHWry+ooTGE2SCp4ufPXC3ZlmhWnpnr2dzpurTqFz1VCrggd/z97wcvz6M0wKq8MrbarzxqqTZGSZeatD3pvAipqRptxDzUizP7UxZcHUjLTiLfX27duqt76IpKSkaIBUZ8dRmmWZZGJKhnrLPkhKhklQSt+HpSHp7tiwYYP44Ycf8qz5qRSeyWTiX//6V1ZSUlIycLrAX1AeWWqm+fsZWy4actbAVe4ymSVzd1wx3TQY04Djzo7HHkp89wKAEKKNTqdbk5GR4avVas0F/8afI6V0AaQQwq7NFSmlixBCAnZvFplMJq1OpzuVnJzcUUp5wd7XK+vctJq/IfhMSlyEoCj/CDVItAiyoEjLdch1TGap9XTVnE3NNHeUUp4tqnKLk1KRdHMIITyxf+u9ObAMaAI83Lp6Dy8Q+F/2NW/Z+VpGKWVmwacpRUVYxhsW5b0IAcQBs7C8R+3tJ2ApsKAIy8ySUmYUfFrJVaqSrr0JITTAduALKeVCB11zNmCWUuadQaEouQghhgIjgDbSAX/YQoimwHqggZRSzdApJJV0H4IQIgJ4A2ghpbR7N0b2NSsBR4BQKeWhgs5XyiYhRHngKNBTSrnbgdf9EkiRUv7dUdcs6VTSLSQhhDeWN/Xzjh7GIoQYCfQAnnFEC0YpeYQQ04DKUsqhBZ5ctNetDBwG2kopjzry2iWVSrqFJIT4AKgrpRzohGu7AvuAd6SUBe/jo5QpQogngJ1AkJTyihOuPxroKKXs5uhrl0Qq6RaCEKI2sBcIllJedFIMzwBzgMDSfqNBeThCiNXATinlVCdd3w04BLwppfzBGTGUJKVhnK4j/AuY7qyECyCl/BmIx9KnrCgACCE6A0HAdGfFkD3qZRTwaXYCVh5AtXQLIIRoD3wNNJRSOnU0uxCiHrADaCylTHBmLIrzCSFcgP3Au1LK1U6ORQA/AD9LKT9zZizFnUq6DyCE0GLpVvhQSvmds+MBEEJ8BPhJKYc5OxbFuYQQrwO9gM7F4QarEKIhlnHCgVLKR1shvQxQSfcBhBAvARFA++Lwpgbr0KBjQA9HDg1SipfiOpRQCDEd8JRSvuzsWIorlXTvQwjhi2WIWJiU8g9nx5ObEGIYEAXoi8uHgeJYQohZAFLK150dS25CiApY/m66SCn3OTue4kgl3fsQQnwKeEspX3J2LPfKnhm3G/hYSvmts+NRHEsI0RjYjOU+w01nx3MvIcTfgBeADqpRkJdKuvkQQjQAtlKM+6aEEHrgWyxTMEvlEnhKXtk3rDYAa6SUMQWd7wzZ90J+Bz6QUi53djzFjRoylr9PganFNeECSCm3AduAMc6ORXGoHkA14AtnB3I/UkoT8CbwcfYiVEouqqV7DyFENyxjHhsX91W3hBC1gD+AZlLKc86OR7EvIYQ7lrHaf5NSbnB2PAURQqwA/pBSTnZ2LMWJSrq5ZA/sPgCMllKuc3Y8hSGEeB9oJKXs7+xYFPsSQrwNtJZS9nR2LIUhhKiL5d7Dk1LKS86Op7hQSTcXIcQo4BmgW0m5ASCE0GEZOjRIShnn7HgU+xBCVAMOAi2llCedHU9hCSGmALWklIOcHUtxoZJutlyrJbWTUh5xdjwPQwjxAvA28Nfs/jSllBFCLACuSylLVB9+9up8x4C+Usodzo6nOFBJN5sQ4t+AQUo5ytmxPKzsO9pbgYVSyrnOjkcpWkKIp4C1QH0pZZKz43lYQohBwEgsrXSHrENdnKmkCwghmmDZeqSBlPK2s+N5FEKI5sA6LH+Yic6ORyka2R+ovwJzpZTznR3Po8geV74DmCOl/NrZ8ThbmR8ylv2mng68X1ITLoCUci+W/dT+6exYlCI1AHADFjo5jkeW3bp9A/hQCFHO2fE4W5lv6Qoh+mJJVM1Ken+oEKIKln7pNlLKY86OR/lzhBBeWKbUviCl/NXZ8fxZQohvgItSynecHYszlemkmz1w+wgwVEr5i7PjKQpCiLeAECnlc86ORflzhBCTgCeklC86O5aiIISogWVI5lNSytPOjsdZynrSfQ9oIqXs6+xYikr2WOPDwEgp5Y/Ojkd5NEKIv2CZSttESnnB2fEUFSHEO1hG2fR2dizOUmaTbq5P3b9KKc84O56iJIToDnyEZVC60dnxKA9PCLEMiJdSTnR2LEVJCOGB5dtllJRys7PjcYYyl3SFEOFALeCvwHkp5btODqnIZd8c/BHLSv5BwDQp5QnnRqUUJHsniB+BScAiLKuIGZwbVdETQvQB3gfeAlpIKT9wckgOVRZHLzQCmgGhwPfZM7pKm6pY9nV7D3gcqOvccJRC8gWaADOwJN6azg2n6GU3CC4BN4G+WF5vmVIWk64v0AnLV5z/ApWdG45dNAKWYrnzXQvLa1aKP1/ADLgDH2CZkl7auACzAFegP+Dn3HAcrywm3b8C1YFzQJCU8qxzwyl6UspNwNPZP9bFkoSV4q864A/osEybLZbr5f4Z2fcYWmHpRvECGjg3Iscri3267wDXysJ02eyZQNOAbVLKNc6OR3kwIURtIBoY5uydpx1BCNEGiJRSjnB2LI5U5pKuoiiKM7nYs3AhhD+WPkVhz+s4yCUp5RVnXTy71VobqOisGIpQFnDamYu3ZA9dehwoDTsbpACnnDk8MHtDytqA1lkxFKFrwAV7Le9qt5auh4fHaCHElBo1aqRrtSX7/4PZbOby5csewByDwTDa0WvtCiE8ypUr94OLi0sLPz8/o+UGcMmVmZkprly54pqRkdFLSvmzo68vhKjn6arZ7uPp4q5z1ZT4Va9SMkyalEzTLUOmuZUzGgYuGjFQqxHzqpZ3y3DRiBL91VkC15ON7hK5JjXTPNAeSwPYJekKITpXrlx51e7du71q1apV5OU7w82bN2nZsqXh5MmTrzp6pSQfH58v27dvH7FixQpPV1dXR17abrZt28YzzzxjSEtLC3DkrgJCCOHpqrkyMay2/8DmVUrNjeRPfrlg/GrHlfik9CyHDsESQgR7u2l3rB3R2LN+5dIx+tKQaeL5r+MN8Qmp0elGc5FvNWSvN91TgwcP9igtCRegUqVKvPbaazovL682jr62i4uLfsyYMaUm4QLo9XqCgoKMQKCDL11Jgm9pSrgAr7ap7pqaaWrshEs3DannayotCRdA56ZlRKtqOp2rtp09yrfXG8/Ny8ur2PQpGI1GIiIi0Ov1TJs2Lc/zmzZtolWrVoSGhnLp0v0bXTqdDldXV4e/u6SUbp6exafrsaD6BEhISMDT05OzZ8/etxwvLy+wjEl1JDc3rShWq8kZTWZGrjhBr3mHmL017/tv6+lEun91kH4LD3MlKSPfMjzdtJilU/pT3XWummLzAVZQXfZdcNj62Hb6/stOe7pqEAIPe8RYbCrLntauXUtQUBDbtm1j27ZtXLtmu7P6lClT2LBhA9HR0fdNIspdBdUnQExMDE8//XQ+v63c6+djt2lQWcfqqMb8dj6ZGym298Nmxl1k6eBGjOtUiznbLjspypKhoLoEWD40kOVDA9HX9XFChE5IurGxsXTt2pWwsDC6dOlCTEwMrVq14t13LUsgzJs3j06dOvHUU0+xZ88eAFauXIler0ev1/P7778/9DV37txJ586dAQgJCbGWC2AwGPD29sbb25sWLVoQHx9fBK/ScYpbfQLcuXOHmzdvUqdOnT/56hxv+5lEBi6KJ2LREV78Jp75O6/Q/auDTN14HoBv916l/9fxdPv3AfZfSgHgh/ib9Jp3iF7zDnHwcspDX/P3C8m0e9ySAFrVLs/+XGWkZZrwctPi5a6l2WPlOH695AzfLW51CSCEpbX7+vITJKVn/clX+Gic0tL19vZm/fr11K5dG4AdO3awZcsWAAYMGMDGjRtZtmwZn376KWazmdmzZ7Nlyxa+//57pkyZYlPWqlWrCAkJsXnMmzfP5pw7d+5Qvnx5AHx8fLhz5471udu3b1ufAzCZitU3z0IpTvUJMHv2bF5++WU7vVr707lpWTyoIY/5Wno+vh8RxM5zltFtvYL8WDakEZ/3C+DLHZcxmyULf0tgxdBAFr7YgJlxtl9p1x+5afOVtu+Cw3y796rNOUnpJrzdLT0D5T20NsngTnoW5dzv9hqYzCVrcEBxqkuAL58PYPnQQDoG+DIrn+4HR7DrON37CQy03DupVq0ajRtb+v49PCzdJ+vWrSMmJgaNRoMQguvXr3P48GE6duxoCdjFNuTw8HDCw8MfeD1fX1+Skiz/o5OSkqhevbr1uQoVKlifAyiJw9uKU32mpaURHx9vbWmXRPX9Ld32Vcq5Ub+K5d/uLpb2ycYTd1iw6wqa7GF7Nw1Gjl9Po//Xlm9IWo3tcL6whpUIa1jpgdcr76ElJcPyYZ+cYaJKOTfrc74eLiRn3G0I3Ft+cVec6hKggs5yM7prw4qsPHDjz7y0R+aUpJt7nOm9Y04/+eQTtmzZwvnz5xkxYgR+fn4EBwezfv16NBoNRqNtH82qVauYMWOGzbFBgwYRFRVl/blVq1Zs2rSJZs2aERsbS0REhPU5nU6HwWAgNTWVw4cP06hRyVumoDjV59mzZzl16hRdu3bl4MGDXL9+nXXr1hXly7W73FV4b4r79/bLrBgayKXEDMasPU1FnSuNquhYHNEQjUZgNNkO+11/5CbzdibYHOvzpB8Dmlex/ty8Zjm2nU4kqLo3O84m0edJf+tznm5a0oxmDJkmjl0zEOBffG6oFkZxqkuAlAxLS3jvhRRqVXD0PVwLpyTdB+nSpQvt2rWjQ4cOgKXl+corrxASEoJWq6VDhw6MHz/een5hWmbdu3dn6NCh6PV6unfvTuXKldm3bx/79u0jMjKSsWPH0rlzZzw8PPjmm2/s+voczRn1uXPnTgAiIyOZMGGC3V6bM4Q87kuf+YdpXcfSvaLVCAY/VZW+Cw+jEYI2dcozKuTuioyFaZ11rl+Bv68+Ra95h+gcUAE/b1cOXUnlcEIq/ZtW5jV9dV74Jh53Fw0zwp+w6+tzJGfU5Qtfx+OqFXi4avi01+N2fX33Y6/JERPef//990vbH9yXX37J22+//Z/bt29HFHx20alYseLJDRs2PN68eXNHXtbuQkNDE3/55ZdBUsrvHXVNIUT18h7aE0fGPV16BpZmq/H+DqSUDu1/EEK8/HwT/08/C3+iVNXnz8duMXr1qa03U41FPla3TAwZUxRFKS5U0lUURXEglXQVRVEcqMQm3cjIyAdOMf0zRo0ahb+/PwsXLrRL+cWRPesz95jfzZvLxgawb646yYXb6XYp+/31Zwmatptlf+SdCVga2bMuL97JYMh/jtJ3wWGW3DPm116K3eiF4uDtt98mODjY2WGUKrGxsc4OodR4TV+dRlVL1X0rp/nX5gtMD3/cOn7XEeyedLdv386oUaPw9PRk0KBBRERE0KNHDwwGA/Xr12fu3LnExsYydepUhBCYzWaee+45lixZQmhoKFOmTCEyMhIvLy8OHDhAx44dbYYhJSQkEBUVRUpKCp06deKf//wn77zzDnFxcWg0GpYuXWozeL8wqlatWsS1UHRKYn1qNBo6dOhA9erVmTNnDj4+zpnznp/d55OZ+ONZPFw19HnSj97B/gxdcpQ0o5nH/Tz5uOfjbD+TyOxtlxAIzFLSKaACqw7eoE0dH8Z2qsWbq06ic9Vw5KoBfV0fRne4O8zpWnImo9ecwpBpQl/Xl1EhjxG98Ty7ziWhETCnbwBVy7s9IMK8Kpd7uPMdpaTVpdFk5lJiBv9Ye5p0o5nJz9ahdkW7rHFjw+5Jd/369bz33nt0794ds9ky2Hnt2rW4u7szePBgjh8/Dlimsi5fvtw6fXTHjh3o9XprOe3atWP27Nl069aNhIS7A6SnTp3KpEmTaN68OQMHDuTKlSv89NNP7Nq1CxcXF+4dEtezZ08SE21XF9q0aVOJmYlWEutz+fLlVKxYkSVLlhAdHc3UqVOLtlL+hF9O3Ob/2tfgmfoVMWdPsV3wYgPcXTT838oTnLphWetA56blq/71GbP2FGCZztpr3iFrOS1ql+fD5+oyaPERriVnWo/P2naJf4TW5Mnq3ry+/ARXkzPZcvIO/xsRhItW5KnPoUuO2sxAA1g2pFGJmIlW0uryliGLI1dT2TqyKddTjXy44Rxf9q9f9BVzD7sn3VdffZVJkybx3XffMXLkSBo2bEhUVBRXr17l3LlzXLliWej+QVNZAZo2bQpAcHCwTd/jsWPHGD16NGBZE+DSpUtMnjyZyMhIKlWqxIcffpizhCAAa9aU7P0ZS2J9Vqxo2WEoPDycxYsX/4lXX/SGPF2Vz2Iv8v2hmwxrUZV6/jpGrznFjZRMLiZmci3F8kf/oOmsAI2rWuqkURUdF+7cXX7x9I10Jv10DoDE9CwSkjIZ07Emo1afpIKnC2M71ULndvcDasGLJXdz3JJWl+U9tAT466jo5UpFL1duGRyzAI7dk26FChX4/PPPuXLlCi+99BJDhgwhODiYd999l4EDB1o/nR40lRVg//79BAQEcODAAd544w3r8YCAAIYPH05QUBAmkwkhBJmZmYSFhREdHc2PP/5Inz59rOeX9JZuSazP5ORkypUrx44dO6hbt26R1UVR8PFwYWr3ulxNzmTM2tP0a+JPoyo63ng+gNeXnyCn8fSg6awA8VcNPO7nSfxVA1Etq1mP163kwYDmlWlYxQuTWSKATJMktF4FYuIu8cvJOzzb6O4sq5Lc0i1pdenpqkXnpiHNaOJOmu3CQvZk96T7xRdfsHr1apKTkxk3bhwtWrRg8uTJ7Nq166HK2bx5MzNnziQ0NNSmz3XcuHGMGDGC1NRUXF1dWbFiBX379sVgMCCEsFkzAArXMps6dSqLFi0CLH2cY8eOfahY7akk1menTp1wc3PD09OTBQsWPFSc9rZoz1V+OnqLlAwTr7etQdPHvJkRd5E/Lj3csoK/nk5k/s4rtKnjY9Pn+nrbGvxj7WkMRhOuGsFX/evz0nfHSDOaEcCAZrZfZwvT0p219RIr9l8H4HqKkdfb1nioWO2lJNbl621r8OI3RzBLmPKsY5YiLRHTgHPm8OcsXegspWUacHGpz9IyDfjNVScZHfIYNSvY/ybMg5SGacDFpS7VNGBFUZRSokSM0y1LkxQcQdVn0Zpeilb+crayUJfFrqUbEhJi92usWrUq361kLl68SGhoKK1bt2bTpk12j8MRVH0Wrb4LDtu1/IbRv1l3RbhtsF3r+HJiBv0WHqbH3INsfcCmiiVFWa3LEtHSLWorV66kZs2aeY5PmzaN6OhoAgMD6dmzp3V3BeXBVH0WncCqXiwfmv+u9HO2XWZcp1rU99cx9NujtHXSxoolRXGtS4e1dEeMGMHJkycB+OCDD9i8eXO+mybmyL0WQE5r7ciRI3Ts2BG9Xs/8+fMfKY7Nmzej1+vR5LNrdHx8PC1atMDb29u6o0RxpeqzaP1jzSnO3LQM3v8s9iLbTifmu3FijtzrAeS02E5cN/D8wsP0mneIpb8/2roIR68ZCJ93iI82nc/z3PHrBpo9Vg4vdy2erlrSMovnfn6qLh/MYUm3d+/erFy5EoC4uDjat2+fZ9PEgrz//vssXryYrVu3smzZMrKy7g5mPnPmTJ4NFUeOHJmnjPnz5zNkyJB8y8+Z4QX5b7hYnKj6LFphjSqy/sgtAHadS6JV7fJ5Nk4syMe/XCSmTz1WDQtk7aEbZJnujgw6fzs9z6aK7607k6eMrSObsHJYIDdSjfxy4rbNc7n3pCzvoSUxvXgmXVWXD+aw7oWOHTsyY8YM+vXrR61atdBqtXk2Tcwt9885w9pOnDjBgAEDALhx4wY3btywjjGtU6dOgYuq7N27l0aNGtnMzMotd2stKSkJX1/fh36djqLqs2jp6/gwb+cVngusRA0fN7QakWfjxNxyH8mpTJD2sgAABCNJREFUzzM303ht+QkAbhmM3DIYreNMa1XwuO9X3dxyb5x45KqBDvUqWJ/LPT8iOcOEj0fxnNCj6vLBHJZ03dzcqFy5MjExMfTu3RvIu2libj4+PiQkJFC1alXregIBAQHMmjULf39/jEYjrq53VwY6c+YMQ4cOtSkjKCiImJgY688HDx5kw4YNxMXFceDAASZNmmSzP1hgYCC7d++mUaNGGAwGdLriu5KTqs+i5eaioZKXK/N3JRCWPavp3o0TcyvnoeVaihF/bzdO37R8Na5byZMpz9ahkpcrRpMZV+3dD53zt9P5++pTNmU0qKxjcq4B+WmZJtxcNGg1gr0XkmlYxcvm/PqVdey7lEKAvydpRjOebsUz6aq6fDCH3kgLDw8nKiqK6OhoIO+mibkNHjyYYcOGERQURLVqlqmAEydOZODAgRiNRipWrMiKFSus5xemZRYZGUlkZCRg6dfMSRBvvvkm06dPZ8yYMQwePJj09HQmTZpUBK/YvlR9Fq2whhV5a80pxnWqBeTdODG3fsH+/H3NKRpU1llbYG91qMnrK05gNEkqeLrw1Qt3Z0gVpnV29nY6b646hc5VQ60KHvw9e1PG8evPMCmsDq+0qc4bq06SkWXmrQ55b1wWJ6ou769EzEgrLkrLjLTiorTMSCsuSsOMtOKiJM5Iy8rIyDAXfFrJkpGRgclkyiz4zCKXlZnpjMvaV3p6OoBjlna6KyvLJIvd+PQ/KyPLjABn/M1lZWSZi77l5mSZWRIpMRZ85sOz15sv/rvvvku7fft2wWeWEGlpaSxatMiQnJz8u6OvbTKZ9i1YsCDTHt9KnOXIkSMcOHDADTjq4EtfzzJL48bjpee9CbB8/3Wzzk1z1gmXjo87nSguJ2YUfGYJkWWSfLfvWnqa0bzXHuXbq3tBeHl5xeh0uqHBwcFZLi4lew6GyWQiPj5em5SU9FNycvLzUkqHjtURQlQoV67ctscee6xmrVq1ZH5LNZYk6enpcvfu3W7p6el/y8rK+sbR1xdCtPRw0WxoXM3L5O2uLdmVCSSmZcmj1wymNKO5tZTymKOv7+GqGeuu1YwPqu5lzH3DqySSUnLmVrrmVmrW/7dvhywRAFEURu9Uq8lkMlm0+BP87wqCzWAXDCYFQZFnMBpkwb3ryjl5YCZ9DI+Z25e3j8uZef3tPbYS3eQrvEnOkxznD3433tAkeUhyPTM7GZustQ6SXCQ5/GntHnhPcjcz97s6wFrrKMlZkv8wi3xOcjMzT7s6wFrrNMlJkr/5pGIzj0muZmYr44WtRReA7/b9BgqwV0QXoEh0AYpEF6BIdAGKRBegSHQBikQXoEh0AYpEF6BIdAGKRBegSHQBikQXoEh0AYpEF6BIdAGKRBegSHQBikQXoEh0AYpEF6BIdAGKRBeg6BNe7afZxee9SgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_avg_error_idx=avg_scores.index(min(avg_scores))\n",
    "\n",
    "\n",
    "print (\"The best Decision tree with minimum average absolute error of {} has maximum tree depth of {}, criterion as \\\"{}\\\", splitter as \\\"{}\\\", and following features in the training set: {}\".\n",
    "       format(min(avg_scores),hyperparams[min_avg_error_idx][0],\n",
    "              hyperparams[min_avg_error_idx][2],hyperparams[min_avg_error_idx][3],\n",
    "              hyperparams[min_avg_error_idx][1]))\n",
    "print (\" \")\n",
    "print (\"Decision tree visualization for the best decision tree:\")\n",
    "print (\" \")\n",
    "\n",
    "X_matrix_opt=train_rows.loc[:,hyperparams[min_avg_error_idx][1]]\n",
    "X_matrix_opt=X_matrix_opt.to_numpy()\n",
    "Y_matrix_opt=train_rows.loc[:,\"Hirability\"]\n",
    "Y_matrix_opt=Y_matrix_opt.to_numpy()\n",
    "X_matrix_opt = np.nan_to_num(X_matrix_opt)\n",
    "\n",
    "\n",
    "best_tree=decision_tree(X_matrix_opt,Y_matrix_opt, tree_depth=hyperparams[min_avg_error_idx][0], plot_tree=True, feature_set=hyperparams[min_avg_error_idx][1], criterion=hyperparams[min_avg_error_idx][2], splitter=hyperparams[min_avg_error_idx][3])\n",
    "\n",
    "\n",
    "\n",
    "# tree.plot_tree(clf)\n",
    "\n",
    "# dot_data = tree.export_graphviz(best_tree, out_file=None, \n",
    "#                       feature_names=hyperparams[min_avg_error_idx][1],  \n",
    "#                       class_names=['1','2','3','4','5'],  \n",
    "#                       filled=True, rounded=True,  \n",
    "#                       special_characters=True)  \n",
    "# graph = graphviz.Source(dot_data)  \n",
    "# graph \n",
    "\n",
    "# from dtreeviz.trees import dtreeviz\n",
    "# db=dtreeviz(best_tree, X_matrix_opt, Y_matrix_opt, target_name='Hirability', feature_names=hyperparams[min_avg_error_idx][1], scale=1.5)\n",
    "\n",
    "# db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Pairwise decision boundary separation of the decision tree using paired features')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = \"rgb\"\n",
    "step = 0.04\n",
    "feature_combos=[['Energy','VoiceProb']]\n",
    "for ix, pairs in enumerate(feature_combos):\n",
    "\n",
    "    plt.subplot(2, 3, ix + 1)\n",
    "    plt.xlabel(feature_combos[ix][0])\n",
    "    plt.ylabel(feature_combos[ix][1])\n",
    "    plt.tight_layout(h_pad=1, w_pad=1, pad=3) \n",
    "\n",
    "    X_matrix_1=train_rows.loc[:,pairs]\n",
    "    X_matrix_1=X_matrix_1.to_numpy()\n",
    "    Y_matrix_1=train_rows.loc[:,\"Hirability\"]\n",
    "    Y_matrix_1=Y_matrix_1.to_numpy()\n",
    "    X_matrix_1 = np.nan_to_num(X_matrix_1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    clf = decision_tree(X_matrix_1,Y_matrix_1, tree_depth=hyperparams[min_avg_error_idx][0], plot_tree=False,\n",
    "                        feature_set=hyperparams[min_avg_error_idx][1],\n",
    "                        criterion=hyperparams[min_avg_error_idx][2], splitter=hyperparams[min_avg_error_idx][3])\n",
    "\n",
    "   \n",
    "\n",
    "    x_min, x_max = X_matrix_1[:, 0].min() - 1,  X_matrix_1[:, 0].max() + 1\n",
    "    y_min, y_max = X_matrix_1[:, 1].min() - 1, X_matrix_1[:, 1].max() + 1\n",
    "    \n",
    "    \n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, step), \n",
    "        np.arange(y_min, y_max, step)\n",
    "    )\n",
    "\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "    \n",
    "    for i, color in zip([3, 4, 5], colors):\n",
    "        ix1 = np.where(Y_matrix_1 == i)\n",
    "        plt.scatter(\n",
    "             X_matrix_1[ix1, 0],\n",
    "             X_matrix_1[ix1, 1],\n",
    "            c=color,\n",
    "            label=['1', '2', '3', '4', '5'],\n",
    "            cmap=plt.cm.RdYlBu,\n",
    "            edgecolor=\"black\",\n",
    "            s=40,\n",
    "        )\n",
    "\n",
    "plt.suptitle(\"Pairwise decision boundary separation of the decision tree using paired features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "The best Decision tree with minimum average absolute error of [0.69230769] has maximum tree depth of 2, criterion as \"mae\", splitter as \"best\", and following features in the training set: ['Energy', 'VoiceProb']\n",
    "\n",
    "The features ['Energy', 'VoiceProb'] has Pearson's correlation coefficient with Hirability as follows:\n",
    "\n",
    "Pearson's correlation coefficient between Hirability and Energy is : 0.4768001512064216\n",
    "\n",
    "Pearson's correlation coefficient between Hirability and VoiceProb is : 0.2872210071588297\n",
    "\n",
    "These are the two highest absolute values of Pearson's correlation coefficient out of the 8 obtained from the 8 different features in the training dataset. Since the Pearson's correlation coefficient is more for these features therefore constructing a decision tree with these features gives the best decision trees.  Also limiting the maximum tree-depth to 2 prevents overfitting in this case. \n",
    "\n",
    "However, since we have very less training data so it is difficult to get a better Decision tree model that performs better on this data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) (3 points) Random forest: Repeat the same task as in question (2) using a random forest. Experiment with the optimal tree depth and number of trees. Compare and contrast the performance of the decision tree with the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing hirability scores for 1 hyperparameter and feature combination:\n",
      " \n",
      "Predicted Hirability score using Random Forest for sample P1 (test data in this fold) with original hirability score of [4] is: [4.4]\n",
      "Predicted Hirability score using Random Forest for sample P2 (test data in this fold) with original hirability score of [5] is: [4.25]\n",
      "Predicted Hirability score using Random Forest for sample P3 (test data in this fold) with original hirability score of [5] is: [4.32]\n",
      "Predicted Hirability score using Random Forest for sample P4 (test data in this fold) with original hirability score of [3] is: [4.85]\n",
      "Predicted Hirability score using Random Forest for sample P5 (test data in this fold) with original hirability score of [4] is: [4.03]\n",
      "Predicted Hirability score using Random Forest for sample P6 (test data in this fold) with original hirability score of [4] is: [4.16]\n",
      "Predicted Hirability score using Random Forest for sample P7 (test data in this fold) with original hirability score of [5] is: [3.98]\n",
      "Predicted Hirability score using Random Forest for sample P8 (test data in this fold) with original hirability score of [4] is: [4.18]\n",
      "Predicted Hirability score using Random Forest for sample P9 (test data in this fold) with original hirability score of [4] is: [4.26]\n",
      "Predicted Hirability score using Random Forest for sample P10 (test data in this fold) with original hirability score of [5] is: [3.96]\n",
      "Predicted Hirability score using Random Forest for sample P11 (test data in this fold) with original hirability score of [5] is: [4.3]\n",
      "Predicted Hirability score using Random Forest for sample P12 (test data in this fold) with original hirability score of [3] is: [4.49]\n",
      "Predicted Hirability score using Random Forest for sample P13 (test data in this fold) with original hirability score of [5] is: [4.2]\n",
      " \n",
      "Average absolute error across all participants using Random forest with no limit on maximum tree depth, number of trees in the forest as 100, and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [0.72]\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.72])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def random_forest(X_train,Y_train, tree_depth=None, n_estimators=100):\n",
    "    if tree_depth:\n",
    "        clf = RandomForestRegressor(max_depth=tree_depth, random_state=0,n_estimators=n_estimators)\n",
    "    else:    \n",
    "        clf = RandomForestRegressor(random_state=0,n_estimators=n_estimators )\n",
    "    clf = clf.fit(X_train,Y_train)\n",
    "    return clf\n",
    "\n",
    "\n",
    "\n",
    "def get_score_rf(train_rows, feature_set=[\"SCL\",\"SCRamp\",\"SCRfreq\",\"HRmean\",\"ACCmean\",\"Energy\",\"ZCR\",\"VoiceProb\"],\n",
    "                 tree_depth=None, print_hirability=False, print_avg_score=True, n_estimators=100):\n",
    "    X_matrix=train_rows.loc[:,feature_set]\n",
    "    X_matrix=X_matrix.to_numpy()\n",
    "    Y_matrix=train_rows.loc[:,\"Hirability\"]\n",
    "    Y_matrix=Y_matrix.to_numpy()\n",
    "    X_matrix = np.nan_to_num(X_matrix)\n",
    "    scores=[]\n",
    "    abs_err=[]\n",
    "#     print (X_matrix)\n",
    "    for i in range(0, len(Y_matrix)):\n",
    "        if (i!=0):\n",
    "            X_train=np.concatenate((X_matrix[:i],X_matrix[i+1:]))\n",
    "            Y_train=np.concatenate((Y_matrix[:i],Y_matrix[i+1:]))\n",
    "            X_test=X_matrix[i:i+1]\n",
    "            Y_test=Y_matrix[i:i+1]\n",
    "        else:\n",
    "            X_train=X_matrix[1:]\n",
    "            Y_train=Y_matrix[1:]\n",
    "            X_test=X_matrix[:1]\n",
    "            Y_test=Y_matrix[:1]\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print (X_train)\n",
    "\n",
    "        clf=random_forest(X_train,Y_train,tree_depth=tree_depth, n_estimators=n_estimators)\n",
    "#         trees.append(clf)\n",
    "        pred=clf.predict(X_test)\n",
    "        if print_hirability:\n",
    "            print (\"Predicted Hirability score using Random Forest for sample P{} (test data in this fold) with original hirability score of {} is: {}\".format(i+1, Y_test, pred))\n",
    "        scores.append(pred)\n",
    "        abs_err.append(abs(pred-Y_test))\n",
    "    \n",
    "    avg_abs_err=sum(abs_err)/len(abs_err)\n",
    "    \n",
    "    if print_avg_score:\n",
    "        print (\" \")\n",
    "        if tree_depth:\n",
    "            print (\"Average absolute error across all participants using Random forest with maximum tree depth of {}, number of trees in the forest as {}, and features in the training set as {} is: {}\".format(tree_depth,n_estimators,feature_set,avg_abs_err))\n",
    "\n",
    "        else:\n",
    "            print (\"Average absolute error across all participants using Random forest with no limit on maximum tree depth, number of trees in the forest as {}, and features in the training set as {} is: {}\".format(n_estimators,feature_set,avg_abs_err))\n",
    "        print (\" \")\n",
    "    return avg_abs_err\n",
    "        \n",
    "# trees=[]\n",
    "print (\"Printing hirability scores for 1 hyperparameter and feature combination:\")\n",
    "print (\" \")\n",
    "get_score_rf(train_rows, print_hirability=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing only Average Absolute error for the rest of the hyperparameter and feature combinations of the Random Forests:\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Random forest with no limit on maximum tree depth, number of trees in the forest as 100, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.69846154]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Random forest with no limit on maximum tree depth, number of trees in the forest as 250, and features in the training set as ['Energy'] is: [0.85846154]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Random forest with maximum tree depth of 2, number of trees in the forest as 200, and features in the training set as ['Energy', 'VoiceProb'] is: [0.69005556]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Random forest with maximum tree depth of 2, number of trees in the forest as 150, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.70850834]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Random forest with maximum tree depth of 3, number of trees in the forest as 400, and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [0.73410829]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Random forest with maximum tree depth of 4, number of trees in the forest as 350, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.71229487]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Random forest with maximum tree depth of 5, number of trees in the forest as 300, and features in the training set as ['Energy'] is: [0.85410256]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Random forest with no limit on maximum tree depth, number of trees in the forest as 10, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.69230769]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Random forest with no limit on maximum tree depth, number of trees in the forest as 15, and features in the training set as ['Energy'] is: [0.88717949]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Random forest with maximum tree depth of 2, number of trees in the forest as 20, and features in the training set as ['Energy', 'VoiceProb'] is: [0.6555235]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Random forest with maximum tree depth of 2, number of trees in the forest as 30, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.67719373]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Random forest with maximum tree depth of 3, number of trees in the forest as 40, and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [0.66987179]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Random forest with maximum tree depth of 4, number of trees in the forest as 45, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.68048433]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Random forest with maximum tree depth of 5, number of trees in the forest as 50, and features in the training set as ['Energy'] is: [0.86282051]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print (\"Printing only Average Absolute error for the rest of the hyperparameter and feature combinations of the Random Forests:\")\n",
    "print (\" \")\n",
    "\n",
    "hyperparams_rf=[(None,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"], 100),\n",
    "             (None,[\"Energy\"], 250),(2,[\"Energy\",\"VoiceProb\"],200),\n",
    "             (2,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"],150),\n",
    "             (3,[\"SCL\",\"SCRamp\",\"SCRfreq\",\"HRmean\",\"ACCmean\",\"Energy\",\"ZCR\",\"VoiceProb\"],400),\n",
    "             (4,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"],350),(5,[\"Energy\"],300),\n",
    "            (None,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"], 10),\n",
    "             (None,[\"Energy\"], 15),(2,[\"Energy\",\"VoiceProb\"], 20),\n",
    "             (2,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"], 30),\n",
    "             (3,[\"SCL\",\"SCRamp\",\"SCRfreq\",\"HRmean\",\"ACCmean\",\"Energy\",\"ZCR\",\"VoiceProb\"], 40),\n",
    "             (4,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"],45),(5,[\"Energy\"], 50)]\n",
    "\n",
    "\n",
    "avg_scores_rf=[]\n",
    "# trees=[]\n",
    "for hyp1 in hyperparams_rf:\n",
    "#     print (hyp[1],hyp[0])\n",
    "    avg_score_rf=get_score_rf(train_rows, feature_set=hyp1[1], tree_depth=hyp1[0],n_estimators=hyp1[2])\n",
    "    avg_scores_rf.append(avg_score_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Random Forest with minimum average absolute error of [0.6555235] has maximum tree depth of 2, number of trees in the forest as 20, and following features in the training set: ['Energy', 'VoiceProb']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=2, n_estimators=20, random_state=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_avg_error_idx_rf=avg_scores_rf.index(min(avg_scores_rf))\n",
    "\n",
    "\n",
    "print (\"The best Random Forest with minimum average absolute error of {} has maximum tree depth of {}, number of trees in the forest as {}, and following features in the training set: {}\".\n",
    "       format(min(avg_scores_rf),hyperparams_rf[min_avg_error_idx_rf][0],\n",
    "              hyperparams_rf[min_avg_error_idx_rf][2],\n",
    "              hyperparams_rf[min_avg_error_idx_rf][1]))\n",
    "\n",
    "\n",
    "\n",
    "X_matrix_opt_1=train_rows.loc[:,hyperparams[min_avg_error_idx][1]]\n",
    "X_matrix_opt_1=X_matrix_opt_1.to_numpy()\n",
    "Y_matrix_opt_1=train_rows.loc[:,\"Hirability\"]\n",
    "Y_matrix_opt_1=Y_matrix_opt_1.to_numpy()\n",
    "X_matrix_opt_1 = np.nan_to_num(X_matrix_opt_1)\n",
    "\n",
    "\n",
    "best_forest=random_forest(X_matrix_opt_1,Y_matrix_opt_1, tree_depth=hyperparams_rf[min_avg_error_idx_rf][0],\n",
    "                          n_estimators=hyperparams_rf[min_avg_error_idx_rf][2])\n",
    "\n",
    "best_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "Following are the results of the experiments performed using Random Forests on the training dataset with various maximum tree depth, number of trees, and feature combinations:\n",
    "\n",
    "1. Average absolute error across all participants using Random forest with no limit on maximum tree depth, number of trees in the forest as 100, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.69846154]\n",
    " \n",
    " \n",
    "2. Average absolute error across all participants using Random forest with no limit on maximum tree depth, number of trees in the forest as 250, and features in the training set as ['Energy'] is: [0.85846154]\n",
    " \n",
    " \n",
    "3. Average absolute error across all participants using Random forest with maximum tree depth of 2, number of trees in the forest as 200, and features in the training set as ['Energy', 'VoiceProb'] is: [0.69005556]\n",
    " \n",
    " \n",
    "4. Average absolute error across all participants using Random forest with maximum tree depth of 2, number of trees in the forest as 150, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.70850834]\n",
    " \n",
    " \n",
    "5. Average absolute error across all participants using Random forest with maximum tree depth of 3, number of trees in the forest as 400, and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [0.73410829]\n",
    " \n",
    " \n",
    "6. Average absolute error across all participants using Random forest with maximum tree depth of 4, number of trees in the forest as 350, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.71229487]\n",
    " \n",
    " \n",
    "7. Average absolute error across all participants using Random forest with maximum tree depth of 5, number of trees in the forest as 300, and features in the training set as ['Energy'] is: [0.85410256]\n",
    " \n",
    " \n",
    "8. Average absolute error across all participants using Random forest with no limit on maximum tree depth, number of trees in the forest as 10, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.69230769]\n",
    " \n",
    " \n",
    "9. Average absolute error across all participants using Random forest with no limit on maximum tree depth, number of trees in the forest as 15, and features in the training set as ['Energy'] is: [0.88717949]\n",
    " \n",
    " \n",
    "10. Average absolute error across all participants using Random forest with maximum tree depth of 2, number of trees in the forest as 20, and features in the training set as ['Energy', 'VoiceProb'] is: [0.6555235]\n",
    " \n",
    " \n",
    "11. Average absolute error across all participants using Random forest with maximum tree depth of 2, number of trees in the forest as 30, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.67719373]\n",
    " \n",
    " \n",
    "12. Average absolute error across all participants using Random forest with maximum tree depth of 3, number of trees in the forest as 40, and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [0.66987179]\n",
    " \n",
    " \n",
    "13. Average absolute error across all participants using Random forest with maximum tree depth of 4, number of trees in the forest as 45, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.68048433]\n",
    " \n",
    " \n",
    "14. Average absolute error across all participants using Random forest with maximum tree depth of 5, number of trees in the forest as 50, and features in the training set as ['Energy'] is: [0.86282051]\n",
    "\n",
    "\n",
    "The best Random Forest with minimum average absolute error of [0.6555235] has maximum tree depth of 2, number of trees in the forest as 20, and following features in the training set: ['Energy', 'VoiceProb']\n",
    "\n",
    "\n",
    "**From the experiments using Random Forests and Decision Trees it can be seen that the on an average the average absolute errors are lesser for Random Forests than Decision Trees i.e, Random Forests are performing better than Decision Trees on an average. The best Random Forest has a minimum average absolute error of 0.6555235 which is lesser than that of the Decision Trees which has a minimum average absolute error of 0.69230769. But the time taken to train Random Forests is greater than Decision tress especially when we are increasing the number of trees in the Random Forest.**\n",
    "\n",
    "\n",
    "**However, since we have very less training data so it is difficult to get a better Random Forest model that performs better on this data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) (1 point) Adaboost: Repeat the same task as in question (2) using Adaboost with a base model of your choice. Compare and contrast the performance of Adaboost method with the decision tree with the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing hirability scores for 1 hyperparameter and feature combination:\n",
      " \n",
      "Predicted Hirability score using Adaboost Regressor for sample P1 (test data in this fold) with original hirability score of [4] is: [4.4]\n",
      "Predicted Hirability score using Adaboost Regressor for sample P2 (test data in this fold) with original hirability score of [5] is: [4.20714286]\n",
      "Predicted Hirability score using Adaboost Regressor for sample P3 (test data in this fold) with original hirability score of [5] is: [4.14444444]\n",
      "Predicted Hirability score using Adaboost Regressor for sample P4 (test data in this fold) with original hirability score of [3] is: [4.65]\n",
      "Predicted Hirability score using Adaboost Regressor for sample P5 (test data in this fold) with original hirability score of [4] is: [3.86666667]\n",
      "Predicted Hirability score using Adaboost Regressor for sample P6 (test data in this fold) with original hirability score of [4] is: [4.]\n",
      "Predicted Hirability score using Adaboost Regressor for sample P7 (test data in this fold) with original hirability score of [5] is: [3.95]\n",
      "Predicted Hirability score using Adaboost Regressor for sample P8 (test data in this fold) with original hirability score of [4] is: [4.1]\n",
      "Predicted Hirability score using Adaboost Regressor for sample P9 (test data in this fold) with original hirability score of [4] is: [4.42069444]\n",
      "Predicted Hirability score using Adaboost Regressor for sample P10 (test data in this fold) with original hirability score of [5] is: [3.95]\n",
      "Predicted Hirability score using Adaboost Regressor for sample P11 (test data in this fold) with original hirability score of [5] is: [4.25]\n",
      "Predicted Hirability score using Adaboost Regressor for sample P12 (test data in this fold) with original hirability score of [3] is: [3.975]\n",
      "Predicted Hirability score using Adaboost Regressor for sample P13 (test data in this fold) with original hirability score of [5] is: [4.25]\n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 1, n_estimators as 50, base_estimator as the obtained best Random Forest, and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [0.68672619]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def AdaBoost(X_train,Y_train, learning_rate=1, n_estimators=50, base_estimator=None):\n",
    "    clf = AdaBoostRegressor(base_estimator=base_estimator,learning_rate=learning_rate,\n",
    "                            random_state=0,n_estimators=n_estimators)\n",
    "    clf = clf.fit(X_train,Y_train)\n",
    "    return clf\n",
    "\n",
    "\n",
    "\n",
    "def get_score_ada(train_rows, feature_set=[\"SCL\",\"SCRamp\",\"SCRfreq\",\"HRmean\",\"ACCmean\",\"Energy\",\"ZCR\",\"VoiceProb\"],\n",
    "                  learning_rate=1, n_estimators=50, base_estimator=None,print_hirability=False, print_avg_score=True):\n",
    "    X_matrix=train_rows.loc[:,feature_set]\n",
    "    X_matrix=X_matrix.to_numpy()\n",
    "    Y_matrix=train_rows.loc[:,\"Hirability\"]\n",
    "    Y_matrix=Y_matrix.to_numpy()\n",
    "    X_matrix = np.nan_to_num(X_matrix)\n",
    "    scores=[]\n",
    "    abs_err=[]\n",
    "    if base_estimator==best_tree:\n",
    "        base_string=\"the obtained best Decision Tree\"\n",
    "    elif base_estimator==best_forest:\n",
    "        base_string=\"the obtained best Random Forest\"\n",
    "    else:\n",
    "        base_string=\"None\"\n",
    "#     print (X_matrix)\n",
    "    for i in range(0, len(Y_matrix)):\n",
    "        if (i!=0):\n",
    "            X_train=np.concatenate((X_matrix[:i],X_matrix[i+1:]))\n",
    "            Y_train=np.concatenate((Y_matrix[:i],Y_matrix[i+1:]))\n",
    "            X_test=X_matrix[i:i+1]\n",
    "            Y_test=Y_matrix[i:i+1]\n",
    "        else:\n",
    "            X_train=X_matrix[1:]\n",
    "            Y_train=Y_matrix[1:]\n",
    "            X_test=X_matrix[:1]\n",
    "            Y_test=Y_matrix[:1]\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print (X_train)\n",
    "\n",
    "        clf=AdaBoost(X_train,Y_train, learning_rate=learning_rate, n_estimators=n_estimators, base_estimator=base_estimator)\n",
    "#         trees.append(clf)\n",
    "        pred=clf.predict(X_test)\n",
    "        if print_hirability:\n",
    "            print (\"Predicted Hirability score using Adaboost Regressor for sample P{} (test data in this fold) with original hirability score of {} is: {}\".format(i+1, Y_test, pred))\n",
    "        scores.append(pred)\n",
    "        abs_err.append(abs(pred-Y_test))\n",
    "    \n",
    "    avg_abs_err=sum(abs_err)/len(abs_err)\n",
    "    \n",
    "    if print_avg_score:\n",
    "        print (\" \")\n",
    "        print (\"Average absolute error across all participants using Adaboost Regressor with learning rate of {}, n_estimators as {}, base_estimator as {}, and features in the training set as {} is: {}\".\n",
    "               format(learning_rate,n_estimators,base_string,feature_set,avg_abs_err))\n",
    "\n",
    "        print (\" \")\n",
    "    return avg_abs_err\n",
    "        \n",
    "# trees=[]\n",
    "print (\"Printing hirability scores for 1 hyperparameter and feature combination:\")\n",
    "print (\" \")\n",
    "adaclf=get_score_ada(train_rows,base_estimator=best_forest, print_hirability=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing only Average Absolute error for the rest of the hyperparameter and feature combinations of the Adaboost Regressors:\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 0.1, n_estimators as 10, base_estimator as None, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.63076923]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 0.15, n_estimators as 15, base_estimator as None, and features in the training set as ['Energy'] is: [0.80769231]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 0.2, n_estimators as 20, base_estimator as the obtained best Random Forest, and features in the training set as ['Energy', 'VoiceProb'] is: [0.67559982]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 0.3, n_estimators as 30, base_estimator as the obtained best Decision Tree, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.76923077]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 0.4, n_estimators as 40, base_estimator as the obtained best Random Forest, and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [0.71456654]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 0.5, n_estimators as 45, base_estimator as the obtained best Decision Tree, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.76923077]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 0.6, n_estimators as 50, base_estimator as the obtained best Random Forest, and features in the training set as ['Energy'] is: [0.66154609]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 0.8, n_estimators as 10, base_estimator as None, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.83333333]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 0.9, n_estimators as 15, base_estimator as None, and features in the training set as ['Energy'] is: [1.04230769]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 1.5, n_estimators as 20, base_estimator as the obtained best Decision Tree, and features in the training set as ['Energy', 'VoiceProb'] is: [0.61538462]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 1, n_estimators as 30, base_estimator as the obtained best Decision Tree, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.92307692]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 1.2, n_estimators as 40, base_estimator as the obtained best Random Forest, and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [0.70592949]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 1.5, n_estimators as 70, base_estimator as the obtained best Decision Tree, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.61538462]\n",
      " \n",
      " \n",
      "Average absolute error across all participants using Adaboost Regressor with learning rate of 0.9, n_estimators as 100, base_estimator as the obtained best Random Forest, and features in the training set as ['Energy'] is: [0.81369963]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print (\"Printing only Average Absolute error for the rest of the hyperparameter and feature combinations of the Adaboost Regressors:\")\n",
    "print (\" \")\n",
    "\n",
    "hyperparams_ada=[(None,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"], 10,0.1),\n",
    "             (None,[\"Energy\"], 15,0.15),(best_forest,[\"Energy\",\"VoiceProb\"], 20,0.2),\n",
    "             (best_tree,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"], 30,0.3),\n",
    "             (best_forest,[\"SCL\",\"SCRamp\",\"SCRfreq\",\"HRmean\",\"ACCmean\",\"Energy\",\"ZCR\",\"VoiceProb\"], 40,0.4),\n",
    "             (best_tree,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"],45, 0.5),(best_forest,[\"Energy\"], 50,0.6),\n",
    "            (None,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"], 10,0.8),\n",
    "             (None,[\"Energy\"], 15,0.9),(best_tree,[\"Energy\",\"VoiceProb\"], 20,1.5),\n",
    "             (best_tree,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"], 30,1),\n",
    "             (best_forest,[\"SCL\",\"SCRamp\",\"SCRfreq\",\"HRmean\",\"ACCmean\",\"Energy\",\"ZCR\",\"VoiceProb\"], 40,1.2),\n",
    "             (best_tree,[\"SCRamp\",\"SCRfreq\",\"Energy\",\"VoiceProb\"],70,1.5),(best_forest,[\"Energy\"], 100,0.9)]\n",
    "\n",
    "\n",
    "avg_scores_ada=[]\n",
    "# trees=[]\n",
    "for hyp2 in hyperparams_ada:\n",
    "#     print (hyp[1],hyp[0])\n",
    "    avg_score_ada=get_score_ada(train_rows, feature_set=hyp2[1], \n",
    "                                learning_rate=hyp2[3], n_estimators=hyp2[2], base_estimator=hyp2[0])\n",
    "    avg_scores_ada.append(avg_score_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Adaboost Regressor with minimum average absolute error of [0.61538462] has learning rate of 1.5, n_estimators as 20, base_estimator as the obtained best Decision Tree, and following features in the training set: ['Energy', 'VoiceProb']\n"
     ]
    }
   ],
   "source": [
    "min_avg_error_idx_ada=avg_scores_ada.index(min(avg_scores_ada))\n",
    "\n",
    "hyp_min=hyperparams_ada[min_avg_error_idx_ada]\n",
    "\n",
    "if hyp_min[0]==best_tree:\n",
    "    base_string=\"the obtained best Decision Tree\"\n",
    "elif hyp_min[0]==best_forest:\n",
    "    base_string=\"the obtained best Random Forest\"\n",
    "else:\n",
    "    base_string=\"None\"\n",
    "\n",
    "\n",
    "print (\"The best Adaboost Regressor with minimum average absolute error of {} has learning rate of {}, n_estimators as {}, base_estimator as {}, and following features in the training set: {}\".\n",
    "       format(min(avg_scores_ada),hyp_min[3],hyp_min[2],base_string,hyp_min[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "Following are the results of the experiments performed using Adaboost Regressors on the training dataset with various learning rates, n_estimators, base estimators and different feature combinations:\n",
    "\n",
    "1. Average absolute error across all participants using Adaboost Regressor with learning rate of 0.1, n_estimators as 10, base_estimator as None, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.63076923]\n",
    " \n",
    " \n",
    "2. Average absolute error across all participants using Adaboost Regressor with learning rate of 0.15, n_estimators as 15, base_estimator as None, and features in the training set as ['Energy'] is: [0.80769231]\n",
    " \n",
    " \n",
    "3. Average absolute error across all participants using Adaboost Regressor with learning rate of 0.2, n_estimators as 20, base_estimator as the obtained best Random Forest, and features in the training set as ['Energy', 'VoiceProb'] is: [0.67559982]\n",
    " \n",
    " \n",
    "4. Average absolute error across all participants using Adaboost Regressor with learning rate of 0.3, n_estimators as 30, base_estimator as the obtained best Decision Tree, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.76923077]\n",
    " \n",
    " \n",
    "5. Average absolute error across all participants using Adaboost Regressor with learning rate of 0.4, n_estimators as 40, base_estimator as the obtained best Random Forest, and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [0.71456654]\n",
    " \n",
    " \n",
    "6. Average absolute error across all participants using Adaboost Regressor with learning rate of 0.5, n_estimators as 45, base_estimator as the obtained best Decision Tree, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.76923077]\n",
    " \n",
    " \n",
    "7. Average absolute error across all participants using Adaboost Regressor with learning rate of 0.6, n_estimators as 50, base_estimator as the obtained best Random Forest, and features in the training set as ['Energy'] is: [0.66154609]\n",
    " \n",
    " \n",
    "8. Average absolute error across all participants using Adaboost Regressor with learning rate of 0.8, n_estimators as 10, base_estimator as None, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.83333333]\n",
    " \n",
    " \n",
    "9. Average absolute error across all participants using Adaboost Regressor with learning rate of 0.9, n_estimators as 15, base_estimator as None, and features in the training set as ['Energy'] is: [1.04230769]\n",
    " \n",
    " \n",
    "10. Average absolute error across all participants using Adaboost Regressor with learning rate of 1.5, n_estimators as 20, base_estimator as the obtained best Decision Tree, and features in the training set as ['Energy', 'VoiceProb'] is: [0.61538462]\n",
    " \n",
    " \n",
    "11. Average absolute error across all participants using Adaboost Regressor with learning rate of 1, n_estimators as 30, base_estimator as the obtained best Decision Tree, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.92307692]\n",
    " \n",
    " \n",
    "12. Average absolute error across all participants using Adaboost Regressor with learning rate of 1.2, n_estimators as 40, base_estimator as the obtained best Random Forest, and features in the training set as ['SCL', 'SCRamp', 'SCRfreq', 'HRmean', 'ACCmean', 'Energy', 'ZCR', 'VoiceProb'] is: [0.70592949]\n",
    " \n",
    " \n",
    "13. Average absolute error across all participants using Adaboost Regressor with learning rate of 1.5, n_estimators as 70, base_estimator as the obtained best Decision Tree, and features in the training set as ['SCRamp', 'SCRfreq', 'Energy', 'VoiceProb'] is: [0.61538462]\n",
    " \n",
    " \n",
    "14. Average absolute error across all participants using Adaboost Regressor with learning rate of 0.9, n_estimators as 100, base_estimator as the obtained best Random Forest, and features in the training set as ['Energy'] is: [0.81369963]\n",
    " \n",
    "The best Adaboost Regressor with minimum average absolute error of [0.61538462] has learning rate of 1.5, n_estimators as 20, base_estimator as the obtained best Decision Tree, and following features in the training set: ['Energy', 'VoiceProb']\n",
    "\n",
    "\n",
    "**We are observing lesser minimum average absolute error for Adaboost Regressor (minimum=0.61538462) than for Decision Trees (minimum=0.69230769) or Random Forests (minimum=0.6555235) i.e, Adaboost Regressors are performing better than Decision Trees or Random Forests. Time taken to train Adaboost Regressor are higher than the time taken to train Decision Trees and Random Forests.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
