{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9dpOSo2ROIh"
   },
   "source": [
    "# Homework_3_432001358_CSCE_633_600\n",
    "## November 12, 2021\n",
    "## CSCE 633 600 (Machine Learning) Homework 3\n",
    "### Name: Rohan Chaudhury\n",
    "### UIN: 432001358\n",
    "\n",
    "\n",
    "### Question 1: Machine learning for facial emotion recognition\n",
    "In this problem, we will process face images coming from the Facial Expression Recognition\n",
    "Challenge (presented in the International Conference of Machine Learning in 2013). The data\n",
    "is uploaded under Homework3 folder in the shared Google Drive. You are given three sets of\n",
    "data: training set (i.e., Q1 Train Data.csv), testing set (i.e., Q1 Test Data.csv), and validation\n",
    "set (i.e., Q1 Validation Data.csv).\n",
    "The data consists of 48X48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount\n",
    "of space in each image. The task is to categorize each face based on the emotion shown in the\n",
    "facial expression in seven categories. More information on the data can also be found in this\n",
    "link.\n",
    "All three files contain two columns:\n",
    "1. The column labeled as \"emotion\" contains the emotion class with numeric code ranging\n",
    "from 0 to 6 (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).\n",
    "2. The column labeled as \"pixels\" contains the 2304 (i.e., 48 X 48) space-separated pixel\n",
    "values of the image in row-wise order, i.e., the first 48 numbers correspond to the first row\n",
    "of the image, the next 48 numbers to the second row of the image, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-13T00:33:16.092355Z",
     "iopub.status.busy": "2021-11-13T00:33:16.092007Z",
     "iopub.status.idle": "2021-11-13T00:33:19.165749Z",
     "shell.execute_reply": "2021-11-13T00:33:19.164700Z",
     "shell.execute_reply.started": "2021-11-13T00:33:16.092319Z"
    },
    "id": "R0bgHtKGR5En",
    "outputId": "4762c63f-d257-421d-bfb2-23b71d9318ab"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('../input/face-emotion-recognition-data/Q1_Train_Data.csv')\n",
    "test_data = pd.read_csv('../input/face-emotion-recognition-data/Q1_Test_Data.csv')\n",
    "validation_data = pd.read_csv('../input/face-emotion-recognition-data/Q1_Validation_Data.csv')\n",
    "\n",
    "print (\" \")\n",
    "print(\"Shape of Train Data: {}\".format(train_data.shape))\n",
    "print (\" \")\n",
    "print(\"Shape of Test Data: {}\".format(test_data.shape))\n",
    "print (\" \")\n",
    "print(\"Shape of Validation Data: {}\".format(validation_data.shape))\n",
    "print (\" \")\n",
    "print(train_data.head)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At65kfeJTvaB"
   },
   "source": [
    "### (a) (1 points) Visualization: Randomly select and visualize 1-2 images per emotion. Note: You can find a useful link on image pre-processing here: https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:20:24.331589Z",
     "iopub.status.busy": "2021-11-12T21:20:24.330293Z",
     "iopub.status.idle": "2021-11-12T21:21:03.749730Z",
     "shell.execute_reply": "2021-11-12T21:21:03.748670Z",
     "shell.execute_reply.started": "2021-11-12T21:20:24.331545Z"
    },
    "id": "TORf7KDgUMxw",
    "outputId": "db26e9f9-14c3-4947-ee4c-f286f6682627"
   },
   "outputs": [],
   "source": [
    "def string_to_int(st):\n",
    "  ar= list((int(x)/255 - 0.5)  for x in st.split(' '))\n",
    "  return ar\n",
    "\n",
    "def get_pixels_int(df):\n",
    "  df['pixels_int']=df.apply(lambda st: np.array(string_to_int(st['pixels'])),axis=1)\n",
    "\n",
    "get_pixels_int(train_data)\n",
    "get_pixels_int(test_data)\n",
    "get_pixels_int(validation_data)\n",
    "\n",
    "\n",
    "print(train_data.head)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-13T00:31:28.720883Z",
     "iopub.status.busy": "2021-11-13T00:31:28.720122Z",
     "iopub.status.idle": "2021-11-13T00:31:28.816884Z",
     "shell.execute_reply": "2021-11-13T00:31:28.815646Z",
     "shell.execute_reply.started": "2021-11-13T00:31:28.720782Z"
    },
    "id": "etilPgn8U2Gr",
    "outputId": "d2c70884-44d7-47f8-c491-d7edc1fb7c00"
   },
   "outputs": [],
   "source": [
    "# plt.rcParams[\"figure.figsize\"] = (2,300)\n",
    "\n",
    "plt.figure(figsize=(400,400))\n",
    "\n",
    "emotion_category={0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral'}\n",
    "\n",
    "emotions= train_data['emotion'].unique().tolist()\n",
    "\n",
    "fig, ax=plt.subplots(len(emotions),2,figsize=(15,15))\n",
    "fig.tight_layout(pad=3.0)\n",
    "# print (emotions)\n",
    "plot_num=0\n",
    "for emotion in emotions:\n",
    "  imgs=train_data[train_data[\"emotion\"]==emotion]\n",
    "  samples=imgs.sample(n=2)\n",
    "  for i in range(samples.shape[0]):\n",
    "    img=samples.iloc[i]\n",
    "    ax[int(plot_num/2),plot_num%2].set_title(\"Emotion label: {}\".format(emotion_category[emotion]))\n",
    "    ax[int(plot_num/2),plot_num%2].imshow(np.array(img['pixels_int'],cmap=plt.get_cmap('gray')).reshape(48,48))\n",
    "    plot_num+=1\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nC0nZdOugCl"
   },
   "source": [
    "### (b) (1 points) Data exploration: Count the number of samples per emotion in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:21:12.900954Z",
     "iopub.status.busy": "2021-11-12T21:21:12.900528Z",
     "iopub.status.idle": "2021-11-12T21:21:12.928861Z",
     "shell.execute_reply": "2021-11-12T21:21:12.927702Z",
     "shell.execute_reply.started": "2021-11-12T21:21:12.900909Z"
    },
    "id": "vl9Cbet3ulX8",
    "outputId": "11377107-0fc1-4795-f040-522489d36c34"
   },
   "outputs": [],
   "source": [
    "samples_count=train_data.groupby(['emotion']).count()\n",
    "samples_count=samples_count[['pixels']].rename({'pixels': 'size'}, axis=1)  \n",
    "samples_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BSVnmbPxPre"
   },
   "source": [
    "### (c) (4 points) Image classification with FNNs: In this part, you will use a feedforward neural network (FNN) (also called multilayer perceptron\") to perform the emotion classification task. The input of the FNN comprises of all the pixels of the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:21:12.931438Z",
     "iopub.status.busy": "2021-11-12T21:21:12.930799Z",
     "iopub.status.idle": "2021-11-12T21:21:17.362078Z",
     "shell.execute_reply": "2021-11-12T21:21:17.361037Z",
     "shell.execute_reply.started": "2021-11-12T21:21:12.931384Z"
    },
    "id": "LSHc5mohvUct"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "train_emotions=np.array(list(train_data['emotion']))\n",
    "train_pixels=np.array(list(train_data['pixels_int']))\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "class TimeHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "        self.epoch_time_start = time.time()\n",
    "    # def on_epoch_begin(self, epoch, logs={}):\n",
    "    #     self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_train_end(self, epoch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "\n",
    "\n",
    "model1 = Sequential([\n",
    "  Dense(784*2, activation='relu', input_shape=(48*48,), name=\"first_hidden_layer\"),\n",
    "  Dense(784, activation='relu', name=\"second_hidden_layer\"),\n",
    "  Dense(784//2, activation='relu', name=\"third_hidden_layer\"),\n",
    "  Dense(784//4, activation='relu', name=\"fourth_hidden_layer\"),\n",
    "  Dense(len(emotions), activation='softmax'),\n",
    "])\n",
    "\n",
    "model2 = Sequential([\n",
    "  Dense(784*2, activation='elu', input_shape=(48*48,), name=\"first_hidden_layer\"),\n",
    "  Dense(784, activation='elu', name=\"second_hidden_layer\"),\n",
    "  Dense(784//2, activation='elu', name=\"third_hidden_layer\"),\n",
    "  Dropout(0.25),\n",
    "  Dense(len(emotions), activation='softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "model3 = Sequential([\n",
    "  Dense(2000, activation='elu', input_shape=(48*48,), name=\"first_hidden_layer\"),\n",
    "  Dense(1000, activation='elu', name=\"second_hidden_layer\"),\n",
    "  Dense(500, activation='elu', name=\"third_hidden_layer\"),\n",
    "  Dropout(0.25),\n",
    "  Dense(len(emotions), activation='softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "model4 = Sequential([\n",
    "  Dense(2000, activation='elu', input_shape=(48*48,), name=\"first_hidden_layer\"),\n",
    "  Dense(1000, activation='elu', name=\"second_hidden_layer\"),\n",
    "  Dense(500, activation='elu', name=\"third_hidden_layer\"),\n",
    "  Dense(250, activation='elu', name=\"fourth_hidden_layer\"),\n",
    "  Dense(784, activation='relu', name=\"fifth_hidden_layer\"),\n",
    "  Dropout(0.25),\n",
    "  Dense(len(emotions), activation='softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "model5 = Sequential([\n",
    "  Dense(2000, activation='elu', input_shape=(48*48,), name=\"first_hidden_layer\", kernel_regularizer=regularizers.l2(0.0001)),\n",
    "  Dense(1000, activation='elu', name=\"second_hidden_layer\", kernel_regularizer=regularizers.l2(0.0001)),\n",
    "  Dense(500, activation='elu', name=\"third_hidden_layer\", kernel_regularizer=regularizers.l2(0.0001)),\n",
    "  Dense(250, activation='elu', name=\"fourth_hidden_layer\", kernel_regularizer=regularizers.l2(0.0001)),\n",
    "  Dropout(0.25),\n",
    "  Dense(len(emotions), activation='softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:21:17.364350Z",
     "iopub.status.busy": "2021-11-12T21:21:17.364036Z",
     "iopub.status.idle": "2021-11-12T21:22:34.120886Z",
     "shell.execute_reply": "2021-11-12T21:22:34.119831Z",
     "shell.execute_reply.started": "2021-11-12T21:21:17.364308Z"
    },
    "id": "LWD-gQCkiMxW",
    "outputId": "7f6200a6-e1bb-44ce-bb90-f06141dab412"
   },
   "outputs": [],
   "source": [
    "time_callback = TimeHistory()\n",
    "\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model3.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model4.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model5.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Train image shape: \", train_pixels.shape)\n",
    "print(train_emotions.shape)\n",
    "\n",
    "time_to_train=[]\n",
    "flatten_train_images = train_pixels\n",
    "\n",
    "history1=model1.fit(np.array(flatten_train_images), to_categorical(train_emotions), epochs=20, batch_size=256,callbacks = [time_callback])\n",
    "time_to_train.append(time_callback.times)\n",
    "\n",
    "\n",
    "history2=model2.fit(np.array(flatten_train_images), to_categorical(train_emotions), epochs=20, batch_size=256,callbacks = [time_callback])\n",
    "time_to_train.append(time_callback.times)\n",
    "history3=model3.fit(np.array(flatten_train_images), to_categorical(train_emotions), epochs=20, batch_size=256,callbacks = [time_callback])\n",
    "time_to_train.append(time_callback.times)\n",
    "history4=model4.fit(np.array(flatten_train_images), to_categorical(train_emotions), epochs=20, batch_size=256,callbacks = [time_callback])\n",
    "time_to_train.append(time_callback.times)\n",
    "history5=model5.fit(np.array(flatten_train_images), to_categorical(train_emotions), epochs=20, batch_size=256,callbacks = [time_callback])\n",
    "time_to_train.append(time_callback.times)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-o6W-MugcRM"
   },
   "source": [
    "### (c.i) (3 points) Experiment on the validation set with different FNN hyper-parameters, e.g.layers, nodes per layer, activation function, dropout, weight regularization, etc. For each hyper-parameter combination that you have used, please report the following: (1) emotion classification accuracy on the training and validation sets; (2) running time for training the FNN; (3) parameters for each FNN. For 2-3 hyper-parameter combinations, please also plot the cross-entropy loss over the number of iterations during training. Note: If running the FNN takes a long time, you can subsample the input images to a smaller size (e.g., 24 x 24)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:22:34.122927Z",
     "iopub.status.busy": "2021-11-12T21:22:34.122577Z",
     "iopub.status.idle": "2021-11-12T21:23:01.855260Z",
     "shell.execute_reply": "2021-11-12T21:23:01.854201Z",
     "shell.execute_reply.started": "2021-11-12T21:22:34.122886Z"
    },
    "id": "Dy-Y_2xCgSQq",
    "outputId": "c566f2dd-53c0-4d8a-b91e-cb9ca93fde53"
   },
   "outputs": [],
   "source": [
    "# print (time_to_train)\n",
    "# for i in time_to_train:\n",
    "#   print (sum(i))\n",
    "\n",
    "validation_emotions=np.array(list(validation_data['emotion']))\n",
    "validation_pixels=np.array(list(validation_data['pixels_int']))\n",
    "\n",
    "flatten_validation_images = validation_pixels\n",
    "validation_performances=[]\n",
    "models=[model1,model2,model3,model4,model5]\n",
    "model_names=['model1','model2','model3','model4','model5']\n",
    "histories=[history1,history2,history3,history4,history5]\n",
    "for i in range(len(models)):\n",
    "  print (\" \")\n",
    "  print (\"Required details for {}\".format(model_names[i]))\n",
    "  print (\" \")\n",
    "  performance1 = models[i].evaluate(flatten_train_images, to_categorical(train_emotions))\n",
    "  print(\"Emotion Classification Accuracy on the Training set: {0}\".format(performance1[1]))\n",
    "  print (\" \")\n",
    "  performance2 = models[i].evaluate(flatten_validation_images, to_categorical(validation_emotions))\n",
    "  validation_performances.append(performance2[1])\n",
    "  print(\"Emotion Classification Accuracy on the Validation set: {0}\".format(performance2[1]))\n",
    "  print (\" \")\n",
    "  print (\"Running time for training the FNN: {} ms\".format(str(time_to_train[i][0])))\n",
    "  print (\" \")\n",
    "  print (\"Parameters for the model:\")\n",
    "  print (\" \")\n",
    "  print (models[i].get_config())\n",
    "  print (\" \")\n",
    "  print (models[i].summary())\n",
    "\n",
    "\n",
    "  print (\" \")\n",
    "  print('Number of Epochs used to train the model: ', len(histories[i].history['loss']))\n",
    "\n",
    "  \n",
    "\n",
    "  print(\" \")\n",
    "  # print(history.history.keys())\n",
    "  # summarize history for loss\n",
    "  plt.plot(histories[i].history['loss'])\n",
    "  plt.title('{} loss vs epochs'.format(model_names[i]))\n",
    "  plt.ylabel('cross-entropy loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train'], loc='upper left')\n",
    "  plt.show()\n",
    "  # summarize history for accuracy\n",
    "  plt.plot(histories[i].history['accuracy'])\n",
    "  plt.title('{} accuracy vs epochs'.format(model_names[i]))\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  print (\" \")\n",
    "  print (\" \")\n",
    "\n",
    "# performance = model1.evaluate(flatten_test_images, to_categorical(validation_emotions))\n",
    "# print(\"Accuracy on Test samples: {0}\".format(performance[1]))\n",
    "# performance = model2.evaluate(flatten_test_images, to_categorical(validation_emotions))\n",
    "# print(\"Accuracy on Test samples: {0}\".format(performance[1]))\n",
    "# performance = model3.evaluate(flatten_test_images, to_categorical(validation_emotions))\n",
    "# print(\"Accuracy on Test samples: {0}\".format(performance[1]))\n",
    "# performance = model4.evaluate(flatten_test_images, to_categorical(validation_emotions))\n",
    "# print(\"Accuracy on Test samples: {0}\".format(performance[1]))\n",
    "# performance = model5.evaluate(flatten_test_images, to_categorical(validation_emotions))\n",
    "# print(\"Accuracy on Test samples: {0}\".format(performance[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lI60v2XMhX5U"
   },
   "source": [
    "### Answer:\n",
    "\n",
    "Five different FNN models have been trained on the training dataset and evaluated on the validation dataset. The required details are shown above. \n",
    "The values in a tabular form is shown below:\n",
    "\n",
    "Model name | Accuracy on Train dataset (%)| Accuracy on Validation dataset (%)| Training Time (ms) | Parameters Count\n",
    "-----------|---------------------------|--------------------------------|---------------|--------------\n",
    "model1          |92.24          |46.86      |12.81  |5,230,463\n",
    "model2         |88.66     |48.59  |11.98    |5,154,807\n",
    "model3      |88.71  |47.20    |12.60    |7,115,007\n",
    "model4      |88.01    |45.03   |13.95    |7,439,029\n",
    "model5        |68.06     |45.11   |14.74  |7,238,507"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0s8QeoWiNFI"
   },
   "source": [
    "### (c.ii) (1 point) Run the best model that was found based on the validation set from question (c.i) on the testing set. Report the emotion classification accuracy on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqs3van0iYNA"
   },
   "source": [
    "### Answer: \n",
    "### The best model that was found based on the validation set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:23:01.860272Z",
     "iopub.status.busy": "2021-11-12T21:23:01.859621Z",
     "iopub.status.idle": "2021-11-12T21:23:01.874894Z",
     "shell.execute_reply": "2021-11-12T21:23:01.873694Z",
     "shell.execute_reply.started": "2021-11-12T21:23:01.860224Z"
    },
    "id": "-hVvsNIFifrh",
    "outputId": "b386d933-07be-442d-b1ec-4b75c921ac95"
   },
   "outputs": [],
   "source": [
    "# print (validation_performances)\n",
    "max_validation=validation_performances.index(max(validation_performances))\n",
    "print (model_names[max_validation])\n",
    "print (models[max_validation].summary())\n",
    "\n",
    "print(\"Emotion Classification Accuracy on the Validation set for the model: {} \".format(validation_performances[max_validation]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lv7ts8GkE50"
   },
   "source": [
    "### The emotion classification accuracy of the model on the testing dataset is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:23:01.877514Z",
     "iopub.status.busy": "2021-11-12T21:23:01.877045Z",
     "iopub.status.idle": "2021-11-12T21:23:02.633405Z",
     "shell.execute_reply": "2021-11-12T21:23:02.632364Z",
     "shell.execute_reply.started": "2021-11-12T21:23:01.877459Z"
    },
    "id": "Gy-e23a2jXvT",
    "outputId": "e9ed5f05-247e-4a5e-d24f-fa3e7bd0eef4"
   },
   "outputs": [],
   "source": [
    "test_emotions=np.array(list(test_data['emotion']))\n",
    "test_pixels=np.array(list(test_data['pixels_int']))\n",
    "\n",
    "flatten_test_images = test_pixels\n",
    "\n",
    "test_performance = models[max_validation].evaluate(flatten_test_images, to_categorical(test_emotions))\n",
    "print(\"Emotion Classification Accuracy on the Testing set: {0}\".format(test_performance[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJNMJ7CeoVms"
   },
   "source": [
    "### (d) (4 points) Image classification with CNNs: In this part, you will use a convolutional neural network (CNN) to perform the emotion classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:23:02.635597Z",
     "iopub.status.busy": "2021-11-12T21:23:02.635281Z",
     "iopub.status.idle": "2021-11-12T21:23:02.891144Z",
     "shell.execute_reply": "2021-11-12T21:23:02.890184Z",
     "shell.execute_reply.started": "2021-11-12T21:23:02.635555Z"
    },
    "id": "LMPHd2kPk08p"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D\n",
    "\n",
    "\n",
    "common_features_1 = [Conv2D(32, kernel_size=3, activation='relu', input_shape=(48,48,1)), \n",
    "            Conv2D(32, kernel_size=3, activation='relu'), \n",
    "            MaxPooling2D(pool_size=(2,2)),\n",
    "            Conv2D(64, kernel_size=3, activation='relu'),\n",
    "            Conv2D(64, kernel_size=3, activation='relu'),\n",
    "            MaxPooling2D(pool_size=(2,2)), Flatten(),]\n",
    "classifier_1 = [Dense(512, activation='relu'), Dense(len(emotions), activation='softmax'),]\n",
    "\n",
    "cnn_model_1 = Sequential(common_features_1+classifier_1)\n",
    "\n",
    "\n",
    "\n",
    "common_features_2 = [Conv2D(64, kernel_size=3, activation='relu', input_shape=(48,48,1),use_bias=True), \n",
    "            Conv2D(64, kernel_size=3, activation='relu',use_bias=True), \n",
    "            MaxPooling2D(pool_size=(2,2)),\n",
    "            Conv2D(128, kernel_size=3, activation='relu',use_bias=True,  kernel_regularizer =tf.keras.regularizers.l2( l=0.001)),\n",
    "            Conv2D(128, kernel_size=3, activation='relu',use_bias=True,  kernel_regularizer =tf.keras.regularizers.l2( l=0.001)),\n",
    "            MaxPooling2D(pool_size=(2,2)), Flatten(),]\n",
    "classifier_2 = [Dense(512, activation='relu',use_bias=True,  kernel_regularizer =tf.keras.regularizers.l2( l=0.01)), Dense(len(emotions), activation='softmax',use_bias=True),]\n",
    "\n",
    "cnn_model_2 = Sequential(common_features_2+classifier_2)\n",
    "\n",
    "\n",
    "common_features_3 = [Conv2D(64, kernel_size=3, activation='elu',input_shape=(48,48,1),use_bias=True), \n",
    "            Conv2D(64, kernel_size=3, activation='elu',padding='same',use_bias=True), \n",
    "            MaxPooling2D(pool_size=(2,2)),\n",
    "            Conv2D(128, kernel_size=3, activation='elu',padding='same',use_bias=True), \n",
    "            MaxPooling2D(pool_size=(2,2)),\n",
    "            Conv2D(128, kernel_size=3, activation='elu', strides=(2, 2),padding='same',use_bias=True,  kernel_regularizer =tf.keras.regularizers.l1( l=0.001)),\n",
    "            Conv2D(128, kernel_size=3, activation='elu', strides=(2, 2),padding='same',use_bias=True, kernel_regularizer =tf.keras.regularizers.l1( l=0.001)),\n",
    "            MaxPooling2D(pool_size=(2,2)), Flatten(),]\n",
    "classifier_3 = [Dense(512, activation='elu',use_bias=True,  kernel_regularizer =tf.keras.regularizers.l1( l=0.01)),Dropout(0.25), Dense(len(emotions), activation='softmax',use_bias=True),]\n",
    "\n",
    "cnn_model_3 = Sequential(common_features_3+classifier_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:23:02.893342Z",
     "iopub.status.busy": "2021-11-12T21:23:02.892999Z",
     "iopub.status.idle": "2021-11-12T21:28:31.519715Z",
     "shell.execute_reply": "2021-11-12T21:28:31.518701Z",
     "shell.execute_reply.started": "2021-11-12T21:23:02.893300Z"
    },
    "id": "ln88TSCfsI7B",
    "outputId": "fecb10ba-6f67-4301-8723-10dab82a57e8"
   },
   "outputs": [],
   "source": [
    "# print(cnn_model_1.summary())  # Compare number of parameteres against FFN\n",
    "\n",
    "time_callback = TimeHistory()\n",
    "cnn_model_1.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'],)\n",
    "cnn_model_2.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'],)\n",
    "cnn_model_3.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'],)\n",
    "time_to_train_cnn=[]\n",
    "\n",
    "train_images_3d = flatten_train_images.reshape(len(flatten_train_images),48,48,1)\n",
    "test_images_3d = flatten_test_images.reshape(len(flatten_test_images),48,48,1)\n",
    "\n",
    "\n",
    "cnn_history_1=cnn_model_1.fit(train_images_3d, to_categorical(train_emotions), epochs=15, batch_size=256,callbacks = [time_callback])\n",
    "time_to_train_cnn.append(time_callback.times)\n",
    "cnn_history_2=cnn_model_2.fit(train_images_3d, to_categorical(train_emotions), epochs=20, batch_size=256,callbacks = [time_callback])\n",
    "time_to_train_cnn.append(time_callback.times)\n",
    "cnn_history_3=cnn_model_3.fit(train_images_3d, to_categorical(train_emotions), epochs=20, batch_size=256,callbacks = [time_callback])\n",
    "time_to_train_cnn.append(time_callback.times)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ntV4gv2yUaQ"
   },
   "source": [
    "### (d.i) (3 points) Experiment on the validation set with different CNN hyper-parameters, e.g. layers, filter size, stride size, activation function, dropout, weight regularization, etc. For each hyper-parameter combination that you have used, please report the following: (1) emotion classification accuracy on the training and validation sets; (2) running time for training the FNN; (3) parameters for each CNN. How do these metrics compare to the FNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:28:31.521811Z",
     "iopub.status.busy": "2021-11-12T21:28:31.521480Z",
     "iopub.status.idle": "2021-11-12T21:28:49.037300Z",
     "shell.execute_reply": "2021-11-12T21:28:49.036231Z",
     "shell.execute_reply.started": "2021-11-12T21:28:31.521768Z"
    },
    "id": "POfgUlhguPO4",
    "outputId": "19dab49f-e038-4018-f2ed-f260532b8835"
   },
   "outputs": [],
   "source": [
    "validation_images_3d = flatten_validation_images.reshape(len(flatten_validation_images),48,48,1)\n",
    "\n",
    "validation_performances_cnn=[]\n",
    "cnn_models=[cnn_model_1,cnn_model_2,cnn_model_3]\n",
    "cnn_model_names=['cnn_model_1','cnn_model_2','cnn_model_3']\n",
    "cnn_histories=[cnn_history_1,cnn_history_2,cnn_history_3]\n",
    "for i in range(len(cnn_models)):\n",
    "  print (\" \")\n",
    "  print (\"Required details for {}\".format(cnn_model_names[i]))\n",
    "  print (\" \")\n",
    "  performance1_cnn = cnn_models[i].evaluate(train_images_3d, to_categorical(train_emotions))\n",
    "  print(\"Emotion Classification Accuracy on the Training set: {0}\".format(performance1_cnn[1]))\n",
    "  print (\" \")\n",
    "  performance2_cnn = cnn_models[i].evaluate(validation_images_3d, to_categorical(validation_emotions))\n",
    "  validation_performances_cnn.append(performance2_cnn[1])\n",
    "  print(\"Emotion Classification Accuracy on the Validation set: {0}\".format(performance2_cnn[1]))\n",
    "  print (\" \")\n",
    "  print (\"Running time for training the FNN: {} ms\".format(str(time_to_train_cnn[i][0])))\n",
    "  print (\" \")\n",
    "  print (\"Parameters for the model:\")\n",
    "  print (\" \")\n",
    "  print (cnn_models[i].get_config())\n",
    "  print (\" \")\n",
    "  print (cnn_models[i].summary())\n",
    "\n",
    "\n",
    "  print (\" \")\n",
    "  print('Number of Epochs used to train the model: ', len(cnn_histories[i].history['loss']))\n",
    "\n",
    "  \n",
    "\n",
    "  print(\" \")\n",
    "  # print(history.history.keys())\n",
    "  # summarize history for loss\n",
    "  plt.plot(cnn_histories[i].history['loss'])\n",
    "  plt.title('{} loss vs epochs'.format(cnn_model_names[i]))\n",
    "  plt.ylabel('cross-entropy loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train'], loc='upper left')\n",
    "  plt.show()\n",
    "  # summarize history for accuracy\n",
    "  plt.plot(cnn_histories[i].history['accuracy'])\n",
    "  plt.title('{} accuracy vs epochs'.format(cnn_model_names[i]))\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  print (\" \")\n",
    "  print (\" \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# performance = cnn_model.evaluate(test_images_3d, to_categorical(test_emotions))\n",
    "\n",
    "# print(\"Accuracy on Test samples: {0}\".format(performance[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "Three different CNN models have been trained on the training dataset and evaluated on the validation dataset. The required details are shown above.\n",
    "The values in a tabular form is shown below:\n",
    "\n",
    "Model name | Accuracy on Train dataset (%)| Accuracy on Validation dataset (%)| Training Time (ms) | Parameters Count\n",
    "-----------|---------------------------|--------------------------------|---------------|--------------\n",
    "cnn_model_1      |99.45       |57.23   |37.71  |2,723,303\n",
    "cnn_model_2         |74.28    |55.75  |85.32    | 5,571,527\n",
    "cnn_model_3     |38.88  |39.73    |91.26  |476,231"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQgYE5ID5oeD"
   },
   "source": [
    "### (d.ii) (1 point) Run the best model that was found based on the validation set from question (d.i) on the testing set. Report the emotion classification accuracy on the testing set. How does this metric compare to the FNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV9wUl8k6B8G"
   },
   "source": [
    "### Answer: \n",
    "### The best model that was found based on the validation set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:28:49.040034Z",
     "iopub.status.busy": "2021-11-12T21:28:49.039396Z",
     "iopub.status.idle": "2021-11-12T21:28:49.058019Z",
     "shell.execute_reply": "2021-11-12T21:28:49.056721Z",
     "shell.execute_reply.started": "2021-11-12T21:28:49.039976Z"
    },
    "id": "xts-0ChA1pb0",
    "outputId": "15607d7b-38f7-4db4-ffd9-8954a847d8e1"
   },
   "outputs": [],
   "source": [
    "max_validation_cnn=validation_performances_cnn.index(max(validation_performances_cnn))\n",
    "print (cnn_model_names[max_validation_cnn])\n",
    "print (cnn_models[max_validation_cnn].summary())\n",
    "\n",
    "print(\"Emotion Classification Accuracy on the Validation set for the cnn model: {} \".format(validation_performances_cnn[max_validation_cnn]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUQbbbSn6nHW"
   },
   "source": [
    "### The emotion classification accuracy of the model on the testing dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:28:49.060501Z",
     "iopub.status.busy": "2021-11-12T21:28:49.059913Z",
     "iopub.status.idle": "2021-11-12T21:28:49.788389Z",
     "shell.execute_reply": "2021-11-12T21:28:49.787334Z",
     "shell.execute_reply.started": "2021-11-12T21:28:49.060454Z"
    },
    "id": "78QZt7ic6Zwc",
    "outputId": "3968a024-f1f4-4b46-f270-78aa595bb1a6"
   },
   "outputs": [],
   "source": [
    "test_images_3d = flatten_test_images.reshape(len(flatten_test_images),48,48,1)\n",
    "\n",
    "test_performance_cnn = cnn_models[max_validation_cnn].evaluate(test_images_3d, to_categorical(test_emotions))\n",
    "print(\"Emotion Classification Accuracy on the Testing set for the CNN: {0}\".format(test_performance_cnn[1]))\n",
    "print (\"  \")\n",
    "print(\"And Emotion Classification Accuracy on the Testing set for the FNN: {0}\".format(test_performance[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbnkwDJv7HEm"
   },
   "source": [
    "### We can see that the Emotion Classification Accuracy on the Testing set for the CNN (57.25%) is better than that of the FNN (47.50%).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d71lR76c8Exk"
   },
   "source": [
    "### (e) (1 point) Bayesian optimization for hyper-parameter tuning: Instead of performing grid or random search to tune the hyper-parameters of the CNN, we can also try a model-based method for finding the optimal hyper-parameters through Bayesian optimization. This method performs a more intelligent search on the hyper-parameter space in order to estimate the best set of hyper-parameters for the data. Use publicly available libraries (e.g., hyperopt in Python) to perform a Bayesian optimization on the hyper-parameter space using the validation set. Report the emotion classification accuracy on the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:28:49.790477Z",
     "iopub.status.busy": "2021-11-12T21:28:49.790168Z",
     "iopub.status.idle": "2021-11-12T21:29:01.383824Z",
     "shell.execute_reply": "2021-11-12T21:29:01.382520Z",
     "shell.execute_reply.started": "2021-11-12T21:28:49.790421Z"
    },
    "id": "w2WLniLM7aeB",
    "outputId": "4c0bc0de-aca6-4c54-c640-4e97dea87beb"
   },
   "outputs": [],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:29:01.387324Z",
     "iopub.status.busy": "2021-11-12T21:29:01.386942Z",
     "iopub.status.idle": "2021-11-12T21:29:02.703073Z",
     "shell.execute_reply": "2021-11-12T21:29:02.702059Z",
     "shell.execute_reply.started": "2021-11-12T21:29:01.387272Z"
    },
    "id": "IvQ6VfVVWnYP"
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:29:02.705150Z",
     "iopub.status.busy": "2021-11-12T21:29:02.704839Z",
     "iopub.status.idle": "2021-11-12T21:39:18.427126Z",
     "shell.execute_reply": "2021-11-12T21:39:18.425879Z",
     "shell.execute_reply.started": "2021-11-12T21:29:02.705107Z"
    },
    "id": "e2yXekXhWvG4",
    "outputId": "95ed1f19-5e51-4381-b7fd-3ab7f035b09f"
   },
   "outputs": [],
   "source": [
    "def optimize_cnn(hyperparameter):\n",
    "  \n",
    "  # Define model using hyperparameters \n",
    "  cnn_model = Sequential([Conv2D(32, kernel_size=hyperparameter['conv_kernel_size'], activation=hyperparameter['activation_fn'], input_shape=(48,48,1)), \n",
    "            Conv2D(32, kernel_size=hyperparameter['conv_kernel_size'], activation=hyperparameter['activation_fn']), \n",
    "            MaxPooling2D(pool_size=(2,2)), Dropout(hyperparameter['dropout_prob']),\n",
    "            Conv2D(64, kernel_size=hyperparameter['conv_kernel_size'], activation=hyperparameter['activation_fn']),\n",
    "            Conv2D(128, kernel_size=hyperparameter['conv_kernel_size'], activation=hyperparameter['activation_fn']), \n",
    "            MaxPooling2D(pool_size=(2,2)), Dropout(hyperparameter['dropout_prob']), \n",
    "            Flatten(),\n",
    "            Dense(512, hyperparameter['activation_fn']), \n",
    "            Dense(len(emotions), activation='softmax'),])\n",
    "  \n",
    "  cnn_model.compile(optimizer=hyperparameter['optimizer'], loss='categorical_crossentropy', metrics=['accuracy'],)\n",
    "\n",
    "\n",
    "  train_X, train_y = train_images_3d, train_emotions\n",
    "  valid_X, valid_y = validation_images_3d, validation_emotions\n",
    "\n",
    "  _ = cnn_model.fit(train_X, to_categorical(train_y), epochs=10, batch_size=256, verbose=0)\n",
    "  # Evaluating accuracy on validation data\n",
    "  performance = cnn_model.evaluate(valid_X, to_categorical(valid_y), verbose=0)\n",
    "\n",
    "  print(\"Hyperparameters: \", hyperparameter, \"Accuracy: \", performance[1])\n",
    "  print(\"----------------------------------------------------\")\n",
    "\n",
    "  return({\"status\": STATUS_OK, \"loss\": -1*performance[1], \"model\":cnn_model})\n",
    "  \n",
    "\n",
    "# Define search space for hyper-parameters\n",
    "space = {\n",
    "    # The kernel_size for convolutions:\n",
    "    'conv_kernel_size': hp.choice('conv_kernel_size', [1, 3, 5]),\n",
    "    # Uniform distribution in finding appropriate dropout values\n",
    "    'dropout_prob': hp.uniform('dropout_prob', 0.1, 0.35),\n",
    "    # Choice of optimizer \n",
    "    'optimizer': hp.choice('optimizer', ['Adam', 'sgd']),\n",
    "    #choice of activation function\n",
    "    'activation_fn': hp.choice('activation', ['relu', 'sigmoid', 'elu']),\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "# Find the best hyperparameters\n",
    "best = fmin(\n",
    "        optimize_cnn,\n",
    "        space,\n",
    "        algo=tpe.suggest,\n",
    "        trials=trials,\n",
    "        max_evals=25,\n",
    "    )\n",
    "\n",
    "print(\"==================================\")\n",
    "print(\"Best Hyperparameters\", best)\n",
    "\n",
    "\n",
    "test_model = trials.results[np.argmin([r['loss'] for r in trials.results])]['model']\n",
    "\n",
    "performance = test_model.evaluate(test_images_3d, to_categorical(test_emotions))\n",
    "\n",
    "print(\"==================================\")\n",
    "print(\"Test Accuracy: \", performance[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2oqXsZDkT48"
   },
   "source": [
    "### The best hypermeters are: {'activation': 'relu', 'conv_kernel_size': 5, 'dropout_prob': 0.20422812367824345, 'optimizer': 'Adam'}\n",
    "\n",
    "\n",
    "### The emotion classification accuracy on the testing set with the best hyperparameters is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T21:39:18.429725Z",
     "iopub.status.busy": "2021-11-12T21:39:18.428823Z",
     "iopub.status.idle": "2021-11-12T21:39:18.936953Z",
     "shell.execute_reply": "2021-11-12T21:39:18.936023Z",
     "shell.execute_reply.started": "2021-11-12T21:39:18.429679Z"
    },
    "id": "Ev57PZvqc-Cz",
    "outputId": "b33bec7f-a88d-4bde-8ac4-dced5c4a6bf2"
   },
   "outputs": [],
   "source": [
    "performance = test_model.evaluate(test_images_3d, to_categorical(test_emotions))\n",
    "\n",
    "print(\"==================================\")\n",
    "print(\"Test Accuracy: \", performance[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4PKdchjkjvh"
   },
   "source": [
    "### (f) (Bonus - 1 point) Fine-tuning: Use a pre-trained CNN (e.g., the pre-trained example of the MNIST dataset that we saw in class, or any other available pre-trained CNN) and fine-tune it on the FER data. Please experiment with different fine-tuning hyper-parameters (e.g., layers to fine-tune, regularization during fine-tuning) on the validation set. Report the classification accuracy for all hyper-parameter combinations on the validation set. Also report the classification accuracy with the best hyper-parameter combination on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T23:23:44.929852Z",
     "iopub.status.busy": "2021-11-12T23:23:44.929469Z",
     "iopub.status.idle": "2021-11-12T23:28:44.391471Z",
     "shell.execute_reply": "2021-11-12T23:28:44.390284Z",
     "shell.execute_reply.started": "2021-11-12T23:23:44.929817Z"
    },
    "id": "PMmxKiyhkjFU",
    "outputId": "775d9dee-6f97-4510-d48c-07e25c8dbb10"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.image import grayscale_to_rgb\n",
    "from tensorflow import convert_to_tensor\n",
    "\n",
    "\n",
    "model_ft_1 = Sequential()\n",
    "model_ft_1.add(ResNet50(input_shape=(48,48,3), include_top=False, pooling='avg', weights=\"imagenet\"))\n",
    "# model.trainable=False\n",
    "model_ft_1.add(Dense(512))\n",
    "model_ft_1.add(Activation('relu'))\n",
    "model_ft_1.add(Dense(1024))\n",
    "model_ft_1.add(Activation('relu'))\n",
    "model_ft_1.add(Dense(512))\n",
    "model_ft_1.add(Activation('relu'))\n",
    "model_ft_1.add(Dropout(0.3))\n",
    "model_ft_1.add(Dense(len(emotions), activation='softmax'))\n",
    "model_ft_1.layers[0].trainable = False\n",
    "model_ft_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model_ft_2 = Sequential()\n",
    "model_ft_2.add( MobileNetV2(input_shape=(48,48,3), include_top=False, pooling='avg', weights=\"imagenet\"))\n",
    "# model.trainable=False\n",
    "model_ft_2.add(Dense(512))\n",
    "model_ft_2.add(Activation('relu'))\n",
    "model_ft_2.add(Dense(1024))\n",
    "model_ft_2.add(Activation('relu'))\n",
    "model_ft_2.add(Dense(512))\n",
    "model_ft_2.add(Activation('relu'))\n",
    "model_ft_2.add(Dropout(0.4))\n",
    "model_ft_2.add(Dense(len(emotions), activation='softmax'))\n",
    "model_ft_2.layers[0].trainable = False\n",
    "model_ft_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_ft_3 = Sequential()\n",
    "model_ft_3 .add( MobileNetV2(input_shape=(48,48,3), include_top=False, pooling='avg', weights=\"imagenet\"))\n",
    "# model.trainable=False\n",
    "model_ft_3 .add(Dense(512, kernel_regularizer=regularizers.l2(0.01)))\n",
    "model_ft_3 .add(Activation('elu'))\n",
    "model_ft_3 .add(Dense(1024, kernel_regularizer=regularizers.l2(0.01)))\n",
    "model_ft_3 .add(Activation('elu'))\n",
    "model_ft_3 .add(Dense(512, kernel_regularizer=regularizers.l2(0.01)))\n",
    "model_ft_3 .add(Activation('elu'))\n",
    "model_ft_3 .add(Dropout(0.3))\n",
    "model_ft_3 .add(Dense(len(emotions), activation='softmax'))\n",
    "model_ft_3 .layers[0].trainable = False\n",
    "model_ft_3 .compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_images_3d_3=grayscale_to_rgb(convert_to_tensor(train_images_3d))\n",
    "\n",
    "history_ft_1 = model_ft_1.fit(train_images_3d_3, to_categorical(train_emotions), epochs=10, batch_size=256, verbose=1)\n",
    "history_ft_2 = model_ft_2.fit(train_images_3d_3, to_categorical(train_emotions), epochs=15, batch_size=256, verbose=1)\n",
    "history_ft_3 = model_ft_3.fit(train_images_3d_3, to_categorical(train_emotions), epochs=15, batch_size=256, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T23:28:44.394235Z",
     "iopub.status.busy": "2021-11-12T23:28:44.393917Z",
     "iopub.status.idle": "2021-11-12T23:29:43.906350Z",
     "shell.execute_reply": "2021-11-12T23:29:43.904379Z",
     "shell.execute_reply.started": "2021-11-12T23:28:44.394187Z"
    },
    "id": "EHi_VSRizYDr"
   },
   "outputs": [],
   "source": [
    "validation_images_3d_3=grayscale_to_rgb(convert_to_tensor(validation_images_3d))\n",
    "\n",
    "# performance = model_ft_3.evaluate(validation_images_3d_3, to_categorical(validation_emotions))\n",
    "\n",
    "# print(\"==================================\")\n",
    "# print(\"Test Accuracy: \", performance[1])\n",
    "\n",
    "\n",
    "validation_performances_ft=[]\n",
    "ft_models=[model_ft_1,model_ft_2,model_ft_3]\n",
    "ft_model_names=['model_ft_1','model_ft_2','model_ft_3']\n",
    "ft_histories=[history_ft_1,history_ft_2,history_ft_3]\n",
    "for i in range(len(ft_models)):\n",
    "  print (\" \")\n",
    "  print (\"Required details for {}\".format(ft_model_names[i]))\n",
    "  print (\" \")\n",
    "  performance1_ft = ft_models[i].evaluate(train_images_3d_3, to_categorical(train_emotions))\n",
    "  print(\"Emotion Classification Accuracy on the Training set: {0}\".format(performance1_ft[1]))\n",
    "  print (\" \")\n",
    "  performance2_ft = ft_models[i].evaluate(validation_images_3d_3, to_categorical(validation_emotions))\n",
    "  validation_performances_ft.append(performance2_ft[1])\n",
    "  print(\"Emotion Classification Accuracy on the Validation set: {0}\".format(performance2_ft[1]))\n",
    "  print (\" \")\n",
    "  print (\"Parameters for the model:\")\n",
    "  print (\" \")\n",
    "  print (ft_models[i].summary())\n",
    "\n",
    "\n",
    "  print (\" \")\n",
    "  print('Number of Epochs used to train the model: ', len(ft_histories[i].history['loss']))\n",
    "\n",
    "  \n",
    "\n",
    "  print(\" \")\n",
    "  # print(history.history.keys())\n",
    "  # summarize history for loss\n",
    "  plt.plot(ft_histories[i].history['loss'])\n",
    "  plt.title('{} loss vs epochs'.format(ft_model_names[i]))\n",
    "  plt.ylabel('cross-entropy loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train'], loc='upper left')\n",
    "  plt.show()\n",
    "  # summarize history for accuracy\n",
    "  plt.plot(ft_histories[i].history['accuracy'])\n",
    "  plt.title('{} accuracy vs epochs'.format(ft_model_names[i]))\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  print (\" \")\n",
    "  print (\" \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "Three different fine-tuned models have been trained on the training dataset and evaluated on the validation dataset. The required details are shown above.\n",
    "The values in a tabular form is shown below:\n",
    "\n",
    "Model name | Accuracy on Train dataset (%)| Accuracy on Validation dataset (%) | Total Trainable Parameters Count\n",
    "-----------|---------------------------|--------------------------------|--------------\n",
    "model_ft_1     |35.55      |34.05   |2,102,791\n",
    "cnn_model_2         |98.22  |42.74  |1,709,575\n",
    "cnn_model_3     |90.38  |40.54  |1,709,575\n",
    "\n",
    "\n",
    "### The best fine-tuned model that was found based on the validation set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T23:29:43.909058Z",
     "iopub.status.busy": "2021-11-12T23:29:43.907973Z",
     "iopub.status.idle": "2021-11-12T23:29:43.937426Z",
     "shell.execute_reply": "2021-11-12T23:29:43.936478Z",
     "shell.execute_reply.started": "2021-11-12T23:29:43.908995Z"
    }
   },
   "outputs": [],
   "source": [
    "max_validation_ft=validation_performances_ft.index(max(validation_performances_ft))\n",
    "print (ft_model_names[max_validation_ft])\n",
    "print (ft_models[max_validation_ft].summary())\n",
    "\n",
    "print(\"Emotion Classification Accuracy on the Validation set for the fine-tuned model: {} \".format(validation_performances_ft[max_validation_ft]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The emotion classification accuracy of the best fine-tuned model (created using mobilenetv2) on the testing dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T23:49:59.918933Z",
     "iopub.status.busy": "2021-11-12T23:49:59.918607Z",
     "iopub.status.idle": "2021-11-12T23:50:01.608101Z",
     "shell.execute_reply": "2021-11-12T23:50:01.606976Z",
     "shell.execute_reply.started": "2021-11-12T23:49:59.918901Z"
    }
   },
   "outputs": [],
   "source": [
    "test_images_3d_3=grayscale_to_rgb(convert_to_tensor(test_images_3d))\n",
    "\n",
    "test_performance_ft= ft_models[max_validation_ft].evaluate(test_images_3d_3, to_categorical(test_emotions))\n",
    "print(\"Emotion Classification Accuracy on the Testing set for the fine-tuned model: {0}\".format(test_performance_ft[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g) (Bonus - 1 point) Data augmentation: Data augmentation is a way to increase the size of our dataset and reduce overfitting, especially when we use complicated models with manyparameters to learn. Using any available toolbox or your own code, implement some of these techniques and augment the original FER data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-13T00:30:27.760737Z",
     "iopub.status.busy": "2021-11-13T00:30:27.760413Z",
     "iopub.status.idle": "2021-11-13T00:30:42.230764Z",
     "shell.execute_reply": "2021-11-13T00:30:42.229584Z",
     "shell.execute_reply.started": "2021-11-13T00:30:27.760704Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "shift=0.2\n",
    "datagen1 = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True, zca_whitening=True, rotation_range=90, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True, vertical_flip=True)\n",
    "datagen2 = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "datagen3 = ImageDataGenerator(zca_whitening=True)\n",
    "datagen4 = ImageDataGenerator(rotation_range=90)\n",
    "datagen5 = ImageDataGenerator(width_shift_range=shift, height_shift_range=shift)\n",
    "\n",
    "datagens=[datagen1, datagen2,datagen3, datagen4, datagen5]\n",
    "labels=[\"Augmentation 1\", \"Augmentation 2\", \"Augmentation 3\",\"Augmentation 4\", \"Augmentation 5\"]\n",
    "for ij,datagen in enumerate(datagens):\n",
    "    datagen.fit(train_images_3d[:9])\n",
    "\n",
    "    for X_batch, y_batch in datagen.flow(train_images_3d, train_emotions, batch_size=9):\n",
    "        for i in range(0, 9):\n",
    "            plt.subplot(330 + 1 + i)\n",
    "            plt.title(labels[ij])\n",
    "            plt.imshow(X_batch[i].reshape(48, 48), cmap=plt.get_cmap('gray'))\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "Some outputs of the following data augmentation techniques are shown above:\n",
    "\n",
    "1. datagen1 = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True, zca_whitening=True, rotation_range=90, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True, vertical_flip=True)\n",
    "2. datagen2 = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "3. datagen3 = ImageDataGenerator(zca_whitening=True)\n",
    "4. datagen4 = ImageDataGenerator(rotation_range=90)\n",
    "5. datagen5 = ImageDataGenerator(width_shift_range=shift, height_shift_range=shift)\n",
    "\n",
    "\n",
    "Now, training a CNN model using datagen1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-13T00:16:08.015520Z",
     "iopub.status.busy": "2021-11-13T00:16:08.015217Z",
     "iopub.status.idle": "2021-11-13T00:28:14.527657Z",
     "shell.execute_reply": "2021-11-13T00:28:14.526379Z",
     "shell.execute_reply.started": "2021-11-13T00:16:08.015489Z"
    }
   },
   "outputs": [],
   "source": [
    "#training a model on augmented dataset\n",
    "datagen1.fit(train_images_3d)\n",
    "it = datagen1.flow(train_images_3d, to_categorical(train_emotions))\n",
    "\n",
    "\n",
    "common_features = [Conv2D(32, kernel_size=3, activation='relu', input_shape=(48,48,1)), \n",
    "            Conv2D(32, kernel_size=3, activation='relu'), \n",
    "            MaxPooling2D(pool_size=(2,2)),\n",
    "            Conv2D(64, kernel_size=3, activation='relu'),\n",
    "            Conv2D(64, kernel_size=3, activation='relu'),\n",
    "            MaxPooling2D(pool_size=(2,2)), Flatten(),]\n",
    "classifier = [Dense(512, activation='relu'), Dense(len(emotions), activation='softmax'),]\n",
    "\n",
    "cnn_model = Sequential(common_features+classifier)\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'],)\n",
    "history_cnn = cnn_model.fit_generator(it, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test dataset using this model is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-13T00:30:42.233471Z",
     "iopub.status.busy": "2021-11-13T00:30:42.233128Z",
     "iopub.status.idle": "2021-11-13T00:30:42.881035Z",
     "shell.execute_reply": "2021-11-13T00:30:42.879999Z",
     "shell.execute_reply.started": "2021-11-13T00:30:42.233414Z"
    }
   },
   "outputs": [],
   "source": [
    "performance = cnn_model.evaluate(test_images_3d, to_categorical(test_emotions))\n",
    "\n",
    "print(\"Accuracy on Test samples: {0}\".format(performance[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
